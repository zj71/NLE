{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"NLE2020_lab_2_2.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"Pq7c607fjXsS"},"source":["# Week 2: Preprocessing Text (Part 2)\n"]},{"cell_type":"code","metadata":{"id":"vo-uSUEwjXsU","executionInfo":{"status":"ok","timestamp":1602732178425,"user_tz":-60,"elapsed":2756,"user":{"displayName":"ZHENYI JIA","photoUrl":"","userId":"01583325167782597357"}}},"source":["#necessary library imports and setup introduced previously\n","\n","import sys\n","from google.colab import drive\n","import re\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from itertools import zip_longest\n","import nltk\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"baDBuSxYlE0Q","executionInfo":{"status":"ok","timestamp":1602732214024,"user_tz":-60,"elapsed":32424,"user":{"displayName":"ZHENYI JIA","photoUrl":"","userId":"01583325167782597357"}},"outputId":"58c8c65a-d58e-46f3-bfb9-67782f66e1d9","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["#mount google drive\n","drive.mount('/content/drive/')\n","#update the system path so that Python knows to look in this folder for libraries\n","sys.path.append('/content/drive/My Drive/NLENotebooks/resources/')\n","from sussex_nltk.corpus_readers import ReutersCorpusReader"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n","Sussex NLTK root directory is /content/drive/My Drive/NLENotebooks/resources\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-7FNGNyljXsY"},"source":["## Overview \n","Remember, a raw text document is just a sequence of characters. There are a number of basic steps that are often performed when processing natural language text. In lab sessions this week we are covering some of the basic text pre-processing methods. In the previous notebook, you looked at\n","- <b> segmentation</b> - breaking down large units of text into smaller units such as documents and sentences. \n","- <b> tokenisation</b> - roughly speaking, this involves grouping characters into words;\n","\n","In this notebook, you will be looking at:\n","- <b>case normalisation</b> - this involves converting all of the text into lower case; \n","- <b>stemming</b> - this involves removing a word's inflections to find the stem; and \n","- <b>punctuation and stop-word removal</b> - stop-words are common functions words that in some situations can be ignored.\n","\n","Note that we do not always apply all of the above preprocessing methods; it depends on the application. One of the things that you will be learning about in this module, is when the application of each of these methods is, and is not, appropriate."]},{"cell_type":"markdown","metadata":{"id":"7l6pjAZljXsY"},"source":["## Normalising text and removing unimportant tokens\n","In this next section we will consider several methods that pre-process (tokenised) text in ways that are sometimes helpful to 'downstream' processing."]},{"cell_type":"markdown","metadata":{"id":"CQbWxHPFjXsZ"},"source":["### Number and case normalisation\n","Without any kind of normalisation, the tokens `\"help\"` and `\"Help\"` are two distinct types. In some contexts you may not want to distinguish them.\n","\n","Another example, is that `\"1998\"` and `\"1999\"` count as distinct types. There are situations where there is no need to distinction between different numbers.\n","\n","The following code performs case normalisation and replaces tokens that consist of digits by \"NUM\". \n","- Python provides a [number of functions](http://docs.python.org/library/stdtypes.html#string-methods), which you can call in order to analyse their content, or produce new strings from them.\n","- The code uses a [list comprehension](http://docs.python.org/tutorial/datastructures.html#list-comprehensions) to build a new list by looping through and filtering items."]},{"cell_type":"code","metadata":{"scrolled":false,"id":"GGESwOoSjXsZ","executionInfo":{"status":"ok","timestamp":1602236172120,"user_tz":-60,"elapsed":1327,"user":{"displayName":"ZHENYI JIA","photoUrl":"","userId":"01583325167782597357"}},"outputId":"2f516853-9d97-42b3-b3fd-65c47660959f","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["tokens = [\"The\",\"cake\",\"is\",\"a\",\"LIE\"]      #a list of tokens, some of which contain uppercase letters\n","print([token.lower() for token in tokens])   #print newly created list of all lowercase tokens\n","\n","numbers = ['in', 'the', 'year', '120', 'of', 'the', 'fourth', 'age', ',', 'after', '120', 'years', 'as', 'king', ',' , 'aragorn', 'died', 'at', 'the', 'age', 'of', '210']\n","print([\"NUM\" if token.isdigit() else token for token in numbers])  #replace all number tokens with \"NUM\" in a new list of tokens"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['the', 'cake', 'is', 'a', 'lie']\n","['in', 'the', 'year', 'NUM', 'of', 'the', 'fourth', 'age', ',', 'after', 'NUM', 'years', 'as', 'king', ',', 'aragorn', 'died', 'at', 'the', 'age', 'of', 'NUM']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"S4uE15AXjXsc"},"source":["### Exercise 1.1\n","- Write a function <code>normalise</code> which \n","    * replaces numbers with NUM; \n","    * and replaces tokens such as `\"4th\"`, `\"1st\"` and `\"22nd\"` with `\"Nth\"`.\n","- Test your code on the list `[\"Within\",\"5\",\"minutes\",\",\",\"the\", \"1st\", \"and\", \"2nd\", \"placed\", \"runners\", \"lapped\", \"the\", \"5th\",\".\"]`. \n","- Check that the token `\"and\"` isn't changed to `\"Nth\"`.\n","- You will find [this page](http://docs.python.org/library/stdtypes.html#string-methods) useful.\n"]},{"cell_type":"code","metadata":{"id":"62J_VD4GjXsd","executionInfo":{"status":"ok","timestamp":1602733066866,"user_tz":-60,"elapsed":1593,"user":{"displayName":"ZHENYI JIA","photoUrl":"","userId":"01583325167782597357"}},"outputId":"103508a5-15ad-4af9-efd5-fa6598ef5462","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["tokens = [\"Within\",\"5\",\"minutes\",\",\",\"the\", \"1st\", \"and\", \"2nd\", \"placed\", \"runners\", \"lapped\", \"the\", \"5th\",\".\"]\n","def normalise(tokens):\n","  normalised=[ \"Nth\" if (token.endswith((\"nd\",\"st\",\"th\"))and token[:-2].isdigit()) else token for token in tokens]\n","  normalised=[ \"NUM\" if token.isdigit() else token for token in normalised]\n","  return normalised\n","\n","print(normalise(tokens))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["['Within', 'NUM', 'minutes', ',', 'the', 'Nth', 'and', 'Nth', 'placed', 'runners', 'lapped', 'the', 'Nth', '.']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pqGsM4K0jXsf"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eTBYKFVVjXsj"},"source":["### Exercise 1.2\n","- Complete the code in the cell below. You have just two lines to complete. The goal is to use a large sample of the Reuters corpus to establish the extent to which vocabulary size is reduced when number and case normalisation is applied.\n","- For each of the two incomplete lines you should use nested list comprehensions. This is described in Section 5.1.4 in [this document](http://docs.python.org/tutorial/datastructures.html#list-comprehensions).  Alternatively, you could define functions which iterate over the sentences in each sample and the tokens within each sentence.\n"]},{"cell_type":"code","metadata":{"id":"JEtQ39z_nEGm","executionInfo":{"status":"ok","timestamp":1602733270171,"user_tz":-60,"elapsed":1692,"user":{"displayName":"ZHENYI JIA","photoUrl":"","userId":"01583325167782597357"}},"outputId":"07cdc832-f4d0-44d1-88eb-4dc8652ad08c","colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["from nltk.tokenize import word_tokenize\n","nltk.download('punkt')"],"execution_count":13,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"qePwmgrnjXsk"},"source":["def vocabulary_size(sentences):\n","    tok_counts = {}\n","    for sentence in sentences: \n","        for token in sentence:\n","            tok_counts[token]=tok_counts.get(token,0)+1\n","    return len(tok_counts.keys())\n","\n","rcr = ReutersCorpusReader()    \n","\n","sample_size = 10000\n","\n","raw_sentences = rcr.sample_raw_sents(sample_size)\n","tokenised_sentences = [word_tokenize(sentence) for sentence in raw_sentences]\n","\n","############################################\n","lowered_sentences = # complete this line\n","normalised_sentences = # complete this line\n","\n","############################################\n","\n","raw_vocab_size = vocabulary_size(tokenised_sentences)\n","normalised_vocab_size = vocabulary_size(normalised_sentences)\n","print(\"Normalisation produced a {0:.2f}% reduction in vocabulary size from {1} to {2}\".format(\n","    100*(raw_vocab_size - normalised_vocab_size)/raw_vocab_size,raw_vocab_size,normalised_vocab_size))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OozqCpzQjXsn"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sXE01afsjXsq"},"source":["## Stemming\n","A considerable amount of the lexical variation found in documents results from the use of morphological variants which we might not wish to distinguish - e.g. when determining the topic of a document. An easy way to remove these varied forms is to use a stemmer. NLTK includes a number of stemmers in the `nltk.stem` package.\n","- [NLTK stem module API](http://nltk.org/api/nltk.stem.html)\n","\n","- [NLTK Porter stemmer](http://nltk.org/api/nltk.stem.html?highlight=stemmer#nltk.stem.porter.PorterStemmer)"]},{"cell_type":"markdown","metadata":{"id":"LYvUBOUnjXsr"},"source":["- Look at the code below to show how the NLTK implementation of the Porter stemmer in `nltk.stem.porter.PorterStemmer` stems a sample of sentences in the Reuters corpus.\n","- Have a close look at the differences between the columns. This will give you a good indication of what the stemmer does."]},{"cell_type":"code","metadata":{"id":"RLMKMiU3jXsr","executionInfo":{"status":"ok","timestamp":1602596953460,"user_tz":-60,"elapsed":3713,"user":{"displayName":"ZHENYI JIA","photoUrl":"","userId":"01583325167782597357"}},"outputId":"a69b0e6a-c403-4a5a-9ee5-df6677c162bc","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from nltk.stem.porter import PorterStemmer\n","\n","rcr = ReutersCorpusReader() \n","st = PorterStemmer()\n","\n","sample_size = 10\n","\n","raw_sentences = rcr.sample_raw_sents(sample_size)\n","tokenised_sentences = [word_tokenize(sentence) for sentence in raw_sentences]\n","\n","for sentence in tokenised_sentences:\n","  df = pd.DataFrame(list(zip_longest(sentence,[st.stem(token) for token in sentence])),columns=[\"BEFORE\",\"AFTER\"])\n","  display(df)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>BEFORE</th>\n","      <th>AFTER</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The</td>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>peso</td>\n","      <td>peso</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>'s</td>\n","      <td>'s</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>effective</td>\n","      <td>effect</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>devaluation</td>\n","      <td>devalu</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>put</td>\n","      <td>put</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>renewed</td>\n","      <td>renew</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>pressure</td>\n","      <td>pressur</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>on</td>\n","      <td>on</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>the</td>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>recently</td>\n","      <td>recent</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>floated</td>\n","      <td>float</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>Thai</td>\n","      <td>thai</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>currency</td>\n","      <td>currenc</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>.</td>\n","      <td>.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         BEFORE    AFTER\n","0           The      the\n","1          peso     peso\n","2            's       's\n","3     effective   effect\n","4   devaluation   devalu\n","5           put      put\n","6       renewed    renew\n","7      pressure  pressur\n","8            on       on\n","9           the      the\n","10     recently   recent\n","11      floated    float\n","12         Thai     thai\n","13     currency  currenc\n","14            .        ."]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>BEFORE</th>\n","      <th>AFTER</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Debt</td>\n","      <td>debt</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>service</td>\n","      <td>servic</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ratio</td>\n","      <td>ratio</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>%</td>\n","      <td>%</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>15.2</td>\n","      <td>15.2</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>13.9</td>\n","      <td>13.9</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>13.3</td>\n","      <td>13.3</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>13.8</td>\n","      <td>13.8</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    BEFORE   AFTER\n","0     Debt    debt\n","1  service  servic\n","2    ratio   ratio\n","3        %       %\n","4     15.2    15.2\n","5     13.9    13.9\n","6     13.3    13.3\n","7     13.8    13.8"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>BEFORE</th>\n","      <th>AFTER</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0830</td>\n","      <td>0830</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Wed</td>\n","      <td>wed</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>UK</td>\n","      <td>UK</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>NON-EU</td>\n","      <td>non-eu</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>TRADE</td>\n","      <td>trade</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>MAY</td>\n","      <td>may</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>N/F</td>\n","      <td>n/f</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>MLN</td>\n","      <td>mln</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>N/A</td>\n","      <td>n/a</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>N/A</td>\n","      <td>n/a</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   BEFORE   AFTER\n","0    0830    0830\n","1     Wed     wed\n","2      UK      UK\n","3  NON-EU  non-eu\n","4   TRADE   trade\n","5     MAY     may\n","6     N/F     n/f\n","7     MLN     mln\n","8     N/A     n/a\n","9     N/A     n/a"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>BEFORE</th>\n","      <th>AFTER</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Philippines</td>\n","      <td>philippin</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>on</td>\n","      <td>on</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>track</td>\n","      <td>track</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>to</td>\n","      <td>to</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>meet</td>\n","      <td>meet</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>growth</td>\n","      <td>growth</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>targets</td>\n","      <td>target</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>-</td>\n","      <td>-</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>govt</td>\n","      <td>govt</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>.</td>\n","      <td>.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        BEFORE      AFTER\n","0  Philippines  philippin\n","1           on         on\n","2        track      track\n","3           to         to\n","4         meet       meet\n","5       growth     growth\n","6      targets     target\n","7            -          -\n","8         govt       govt\n","9            .          ."]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>BEFORE</th>\n","      <th>AFTER</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>New</td>\n","      <td>new</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>York</td>\n","      <td>york</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>City</td>\n","      <td>citi</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>,</td>\n","      <td>,</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>which</td>\n","      <td>which</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>started</td>\n","      <td>start</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>selecting</td>\n","      <td>select</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>investment</td>\n","      <td>invest</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>bankers</td>\n","      <td>banker</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>for</td>\n","      <td>for</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>lucrative</td>\n","      <td>lucr</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>spots</td>\n","      <td>spot</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>on</td>\n","      <td>on</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>its</td>\n","      <td>it</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>underwriting</td>\n","      <td>underwrit</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>team</td>\n","      <td>team</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>for</td>\n","      <td>for</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>a</td>\n","      <td>a</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>new</td>\n","      <td>new</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>agency</td>\n","      <td>agenc</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>,</td>\n","      <td>,</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>emphasized</td>\n","      <td>emphas</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>that</td>\n","      <td>that</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>the</td>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>companies</td>\n","      <td>compani</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>and</td>\n","      <td>and</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>top</td>\n","      <td>top</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>officials</td>\n","      <td>offici</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>must</td>\n","      <td>must</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>have</td>\n","      <td>have</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>paid</td>\n","      <td>paid</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>their</td>\n","      <td>their</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>taxes</td>\n","      <td>tax</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>.</td>\n","      <td>.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          BEFORE      AFTER\n","0            New        new\n","1           York       york\n","2           City       citi\n","3              ,          ,\n","4          which      which\n","5        started      start\n","6      selecting     select\n","7     investment     invest\n","8        bankers     banker\n","9            for        for\n","10     lucrative       lucr\n","11         spots       spot\n","12            on         on\n","13           its         it\n","14  underwriting  underwrit\n","15          team       team\n","16           for        for\n","17             a          a\n","18           new        new\n","19        agency      agenc\n","20             ,          ,\n","21    emphasized     emphas\n","22          that       that\n","23           the        the\n","24     companies    compani\n","25           and        and\n","26           top        top\n","27     officials     offici\n","28          must       must\n","29          have       have\n","30          paid       paid\n","31         their      their\n","32         taxes        tax\n","33             .          ."]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>BEFORE</th>\n","      <th>AFTER</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>--</td>\n","      <td>--</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Abidjan</td>\n","      <td>abidjan</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>newsroom</td>\n","      <td>newsroom</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>,</td>\n","      <td>,</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>+</td>\n","      <td>+</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>225</td>\n","      <td>225</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>21</td>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>90</td>\n","      <td>90</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>90</td>\n","      <td>90</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     BEFORE     AFTER\n","0        --        --\n","1   Abidjan   abidjan\n","2  newsroom  newsroom\n","3         ,         ,\n","4         +         +\n","5       225       225\n","6        21        21\n","7        90        90\n","8        90        90"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>BEFORE</th>\n","      <th>AFTER</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4.First</td>\n","      <td>4.first</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Tennessee</td>\n","      <td>tennesse</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5.2520</td>\n","      <td>5.2520</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      BEFORE     AFTER\n","0    4.First   4.first\n","1  Tennessee  tennesse\n","2     5.2520    5.2520"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>BEFORE</th>\n","      <th>AFTER</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>On</td>\n","      <td>On</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>the</td>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>government</td>\n","      <td>govern</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>bond</td>\n","      <td>bond</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>market</td>\n","      <td>market</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>there</td>\n","      <td>there</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>was</td>\n","      <td>wa</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>a</td>\n","      <td>a</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>mixed</td>\n","      <td>mix</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>reaction</td>\n","      <td>reaction</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>.</td>\n","      <td>.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        BEFORE     AFTER\n","0           On        On\n","1          the       the\n","2   government    govern\n","3         bond      bond\n","4       market    market\n","5        there     there\n","6          was        wa\n","7            a         a\n","8        mixed       mix\n","9     reaction  reaction\n","10           .         ."]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>BEFORE</th>\n","      <th>AFTER</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>02/01/2010</td>\n","      <td>02/01/2010</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>300M</td>\n","      <td>300m</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5.45</td>\n","      <td>5.45</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>%</td>\n","      <td>%</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       BEFORE       AFTER\n","0  02/01/2010  02/01/2010\n","1        300M        300m\n","2        5.45        5.45\n","3           %           %"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>BEFORE</th>\n","      <th>AFTER</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Municipal</td>\n","      <td>municip</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>bond</td>\n","      <td>bond</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>sales</td>\n","      <td>sale</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>for</td>\n","      <td>for</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Tuesday</td>\n","      <td>tuesday</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>are</td>\n","      <td>are</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>expected</td>\n","      <td>expect</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>to</td>\n","      <td>to</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>total</td>\n","      <td>total</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>$</td>\n","      <td>$</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>billion</td>\n","      <td>billion</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>in</td>\n","      <td>in</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>49</td>\n","      <td>49</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>sales</td>\n","      <td>sale</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>,</td>\n","      <td>,</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>according</td>\n","      <td>accord</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>to</td>\n","      <td>to</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>Reuters</td>\n","      <td>reuter</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Municipal</td>\n","      <td>municip</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>Bond</td>\n","      <td>bond</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>Information</td>\n","      <td>inform</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>Group</td>\n","      <td>group</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>.</td>\n","      <td>.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         BEFORE    AFTER\n","0     Municipal  municip\n","1          bond     bond\n","2         sales     sale\n","3           for      for\n","4       Tuesday  tuesday\n","5           are      are\n","6      expected   expect\n","7            to       to\n","8         total    total\n","9             $        $\n","10            2        2\n","11      billion  billion\n","12           in       in\n","13           49       49\n","14        sales     sale\n","15            ,        ,\n","16    according   accord\n","17           to       to\n","18      Reuters   reuter\n","19    Municipal  municip\n","20         Bond     bond\n","21  Information   inform\n","22        Group    group\n","23            .        ."]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"ZaBir7EVjXsw"},"source":["### Exercise 2.1\n","- By looking at the impact on a large sample of the Reuters corpus, establish the extent to which vocabulary size is reduced by stemming.\n","- Write code to do this in the empty cell below. You should be able to re-use a lot of the code from the code you used when measuring the impact of lower case and number normalisation."]},{"cell_type":"code","metadata":{"id":"laXsH-MZjXsx","executionInfo":{"status":"error","timestamp":1602733985319,"user_tz":-60,"elapsed":1325,"user":{"displayName":"ZHENYI JIA","photoUrl":"","userId":"01583325167782597357"}},"outputId":"ee5b1f6e-16bd-4508-da69-be75b92aaa3c","colab":{"base_uri":"https://localhost:8080/","height":241}},"source":["sample_size=1000\n","raw_sentences = rcr.sample_raw_sents(sample_size)\n","tokensized_sentences = [word_tokenize(sentence) for sentence in raw_sentences]\n","normalised_sentences = [normalise(sentence)for sentence in tokenised_sentences]\n","stemmed_sentences = [st.stem(token)for sentence in normalised_sentences]\n","raw_vocab_size = vocabulary_size(tokenised_sentences)\n","stemmed_vocab_size = vocabulary_size(stemmed_sentences)\n","print(\"stemming produced a {0:.2f}% reduction in vocabulary size from {1} to {2}}\".format(100*(raw_vocab_size - stemmed_vocab_size)/raw_vocab_size,raw_vocab_size,stemmed_vocab_size))"],"execution_count":14,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-ca944f6f98c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mraw_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrcr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_raw_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtokensized_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_sentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnormalised_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnormalise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenised_sentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstemmed_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnormalised_sentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'rcr' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"ZzA9Cl6ZjXsz"},"source":["### Exercise 2.2\n","* Try using the WordNetLemmatizer <code>nltk.stem.wordnet.WordNetLemmatizer</code> instead of the Porter Stemmer.\n","* Using a large sample of the Reuters corpus, establish the extent to which the vocabulary size reduced by lemmatization?\n","* As an extension, you could look at different sample sizes and/or different corpora and display the results in a table or graph (using <code>pandas</code> and <code>matplotlib</code>)"]},{"cell_type":"code","metadata":{"id":"NBLtSKiimwG9","executionInfo":{"status":"ok","timestamp":1602597338972,"user_tz":-60,"elapsed":1727,"user":{"displayName":"ZHENYI JIA","photoUrl":"","userId":"01583325167782597357"}},"outputId":"e075fb84-ec68-49e3-867b-3b66ade15509","colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["from nltk.stem.wordnet import WordNetLemmatizer\n","nltk.download('wordnet')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"4_RtCin6jXsz"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kBwfUlC3jXs1"},"source":["### Punctuation and stop-word removal\n","A stopword is a word that occurs so often that it loses its usefulness in some tasks. We may get more meaningful information from our corpus analysis if we remove stopwords and punctuation.\n","\n","The code below takes a list of tokens and creates a new list, which contains only those strings which are alphabetic and non-stop-words.\n","停用词是一个经常出现的单词，在某些任务中失去其用处。 如果我们删除停用词和标点符号，我们可能会从语料库分析中获得更多有意义的信息。\n","\n","下面的代码获取一个令牌列表并创建一个新列表，该列表仅包含字母和非停用词的字符串。"]},{"cell_type":"code","metadata":{"id":"2suL2eJOmqvW","executionInfo":{"status":"ok","timestamp":1602597480149,"user_tz":-60,"elapsed":1562,"user":{"displayName":"ZHENYI JIA","photoUrl":"","userId":"01583325167782597357"}},"outputId":"590b4dbe-c9bb-457f-d818-39277a8924be","colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["from nltk.corpus import stopwords\n","nltk.download('stopwords')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"mhTSVWNcjXs2","executionInfo":{"status":"ok","timestamp":1602597482399,"user_tz":-60,"elapsed":1085,"user":{"displayName":"ZHENYI JIA","photoUrl":"","userId":"01583325167782597357"}},"outputId":"1a272092-465a-4a09-d4b2-85ecc5f30b53","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["\n","stop = stopwords.words('english')\n","tokens=\"The cat , which is really fat , sat on the mat\".lower().split()\n","filtered_tokens = [w for w in tokens if w.isalpha() and w not in stop]\n","print(tokens)\n","print(filtered_tokens)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['the', 'cat', ',', 'which', 'is', 'really', 'fat', ',', 'sat', 'on', 'the', 'mat']\n","['cat', 'really', 'fat', 'sat', 'mat']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QGATIwNmjXs5"},"source":["**Note**: `isalpha` only returns `True` if the string is entirely composed of alphabet characters. If you want a function to return `True` even when a word contains digits, then you should use `isalnum`.`"]},{"cell_type":"markdown","metadata":{"id":"_XyRZmVnjXs5"},"source":["### Exercise 3.1\n","- In the empty cell below, write code that looks at a large sample of the Medline corpus, establishing what proportion of tokens are stop-words.\n","- As extension, you could establish the mean (and or the distribution of the) number of stop-words per sentence; or compare the numbers of stop-words in different corpora."]},{"cell_type":"code","metadata":{"id":"scuIYe_xjXs6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sUUOjqB9nRSp"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ba2ncBYZjXs9"},"source":[""],"execution_count":null,"outputs":[]}]}