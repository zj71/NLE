{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Lab_3_1_SOLUTIONS.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"swLDp-GV7iQg"},"source":["# Week 3: Basic Document Classification (Part 1)"]},{"cell_type":"markdown","metadata":{"id":"-7ksrUf67iQh"},"source":["## Preliminaries "]},{"cell_type":"code","metadata":{"id":"D7xDsFhC7iQi","executionInfo":{"status":"ok","timestamp":1603286829299,"user_tz":-60,"elapsed":86649,"user":{"displayName":"ZHENYI JIA","photoUrl":"","userId":"01583325167782597357"}},"outputId":"5f881618-b4d5-4276-85f2-21fc2457e048","colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["#necessary library imports and setup introduced previously\n","\n","from google.colab import drive\n","#mount google drive\n","drive.mount('/content/drive/')\n","\n","import sys\n","#sys.path.append(r'T:\\Departments\\Informatics\\LanguageEngineering') \n","#sys.path.append(r'\\\\ad.susx.ac.uk\\ITS\\TeachingResources\\Departments\\Informatics\\LanguageEngineering\\resources')\n","#sys.path.append(r'/Users/juliewe/resources')\n","sys.path.append('/content/drive/My Drive/NLENotebooks/resources/')\n","\n","import re\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from itertools import zip_longest\n","from nltk.tokenize import word_tokenize\n","\n","from sussex_nltk.corpus_readers import ReutersCorpusReader"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n","Sussex NLTK root directory is /content/drive/My Drive/NLENotebooks/resources\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B1L5rIQK-ZL3","executionInfo":{"status":"ok","timestamp":1603286830833,"user_tz":-60,"elapsed":88158,"user":{"displayName":"ZHENYI JIA","photoUrl":"","userId":"01583325167782597357"}},"outputId":"0cf14eac-cc6e-4a6b-d8cf-20820a59c7e5","colab":{"base_uri":"https://localhost:8080/","height":101}},"source":["#download nltk resources\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"aLh_fS4P7iQn"},"source":["## Overview \n","In labs this week (and next), the focus will be on the application of sentiment analysis. You will be using a corpus of **book reviews** within an **Amazon review corpus**.\n","\n","You will be exploring various techniques that can be used to classify the sentiment of Amazon book reviews as either positive or negative. \n","\n","You will be developing your own **Word List** and **Naïve Bayes** classifiers and then comparing them to the **NLTK Naïve Bayes** classifier."]},{"cell_type":"markdown","metadata":{"id":"1gUYw61N7iQo"},"source":["## Creating training and testing sets\n","You will be training and testing various document classifiers. It is essential that the data used in the testing phase is not used during the training phase, since this can lead to overestimating performance. \n","\n","We now introduce the `split_data` function (defined in the cell below) which can be used to get separate **training** and **testing** sets.\n","\n","> Look through the code in the following cell, reading the comments and making sure that you understand each line."]},{"cell_type":"code","metadata":{"id":"HsMcMo5e7iQp"},"source":["from random import sample # have a look at https://docs.python.org/3/library/random.html to see what random.sample does\n","from sussex_nltk.corpus_readers import AmazonReviewCorpusReader\n","\n"," \n","def split_data(data, ratio=0.7): # when the second argument is not given, it defaults to 0.7\n","    \"\"\"\n","    Given corpus generator and ratio:\n","     - partitions the corpus into training data and test data, where the proportion in train is ratio,\n","\n","    :param data: A corpus generator.\n","    :param ratio: The proportion of training documents (default 0.7)\n","    :return: a pair (tuple) of lists where the first element of the \n","            pair is a list of the training data and the second is a list of the test data.\n","    \"\"\"\n","    \n","    data = list(data) # data is a generator, so this puts all the generated items in a list\n"," \n","    n = len(data)  #Found out number of samples present\n","    train_indices = sample(range(n), int(n * ratio))          #Randomly select training indices\n","    test_indices = list(set(range(n)) - set(train_indices))   #Other items are testing indices\n"," \n","    train = [data[i] for i in train_indices]           #Use training indices to select data\n","    test = [data[i] for i in test_indices]             #Use testing indices to select data\n"," \n","    return (train, test)                       #Return split data\n"," "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iu1pTJhr7iQu"},"source":["Now we can use this function together with a <code>reader</code> object  to create training and testing data.  Note that the <code>AmazonReviewCorpusReader().category(\"dvd\")</code> returns a reader over just the *dvd* reviews.  The methods <code>positive()</code>, <code>negative()</code> can be called to create readers over reviews classified accordingly to their sentiment.  "]},{"cell_type":"code","metadata":{"id":"QUVZGOpJ7iQv"},"source":["#Create an Amazon corpus reader pointing at only dvd reviews\n","dvd_reader = AmazonReviewCorpusReader().category(\"dvd\")\n","\n","#The following two lines use the documents function on the Amazon corpus reader. \n","#This returns a generator over reviews in the corpus. \n","#Each review is an instance of a Python class called AmazonReview. \n","#An AmazonReview object contains all the data about a review.\n","dvd_pos_train, dvd_pos_test = split_data(dvd_reader.positive().documents())\n","dvd_neg_train, dvd_neg_test = split_data(dvd_reader.negative().documents())\n","\n","#You can also combine the training data\n","dvd_train = dvd_pos_train + dvd_neg_train\n","dvd_test=dvd_pos_test + dvd_neg_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yP5wctINNDUS"},"source":["dvd_pos_test[0].words()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aJaeMrs87iQy"},"source":["### Exercise 1.1\n","* Generate 80:20 *training*:*testing* splits of all 4 categories of reviews (*dvd*, *book*, *kitchen* and *electronics*), containing **positive** and **negative** reviews.  \n","* Record the number of reviews according to category, sentiment and dataset (training or testing) in a Pandas dataframe\n","* Answer the following questions:\n","    1. Regarding the *training* data for *books*, how many are a) **positive**, b) **negative**?\n","    2. Regarding the **negative** *testing* data for, how many are there for each category: a) *dvd*, b) *book*, c) *kitchen* and d) *electronics*? "]},{"cell_type":"code","metadata":{"id":"gTBJnoR8GNsE","executionInfo":{"status":"ok","timestamp":1603286880597,"user_tz":-60,"elapsed":1329,"user":{"displayName":"ZHENYI JIA","photoUrl":"","userId":"01583325167782597357"}},"outputId":"d6bc8a88-4024-48ff-c4f8-47c7799589e1","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["dvd_reader = AmazonReviewCorpusReader().category(\"dvd\")\n","dvd_pos_train, dvd_pos_test = split_data(dvd_reader.positive().documents(),ratio=0.8)\n","dvd_neg_train, dvd_neg_test = split_data(dvd_reader.negative().documents(),ratio=0.8)\n","print(len(dvd_pos_train))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["800\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NPPHBnNcNjGQ","executionInfo":{"status":"ok","timestamp":1603183994423,"user_tz":-60,"elapsed":769,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"7eb200f5-aab3-48ac-e270-7bda47fce244","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(len(dvd_neg_test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["200\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4RT9emZpGhjb"},"source":["How can we make this better?"]},{"cell_type":"code","metadata":{"id":"_HKPDlse7iQz","executionInfo":{"status":"ok","timestamp":1603286891662,"user_tz":-60,"elapsed":6148,"user":{"displayName":"ZHENYI JIA","photoUrl":"","userId":"01583325167782597357"}},"outputId":"f88b8cd5-1e3a-4dd8-dd01-747e8e1d18c8","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["categories=[\"dvd\",\"kitchen\",\"electronics\",\"book\"]\n","\n","numbers=[]\n","\n","for c in categories:\n","  c_reader= AmazonReviewCorpusReader().category(c)\n","  pos_train, pos_test = split_data(c_reader.positive().documents(),ratio=0.8)\n","  numbers.append((c,\"positive\",\"train\",len(pos_train)))\n","  numbers.append((c,\"positive\",\"test\",len(pos_test)))\n","  neg_train, neg_test = split_data(c_reader.negative().documents(),ratio=0.8)\n","  numbers.append((c,\"negative\",\"train\",len(neg_train)))\n","  numbers.append((c,\"negative\",\"test\",len(neg_test)))\n","\n","print(numbers)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[('dvd', 'positive', 'train', 800), ('dvd', 'positive', 'test', 200), ('dvd', 'negative', 'train', 800), ('dvd', 'negative', 'test', 200), ('kitchen', 'positive', 'train', 800), ('kitchen', 'positive', 'test', 200), ('kitchen', 'negative', 'train', 800), ('kitchen', 'negative', 'test', 200), ('electronics', 'positive', 'train', 800), ('electronics', 'positive', 'test', 200), ('electronics', 'negative', 'train', 800), ('electronics', 'negative', 'test', 200), ('book', 'positive', 'train', 800), ('book', 'positive', 'test', 200), ('book', 'negative', 'train', 800), ('book', 'negative', 'test', 200)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tE_JGdg2Glt3","executionInfo":{"status":"ok","timestamp":1603286918845,"user_tz":-60,"elapsed":2201,"user":{"displayName":"ZHENYI JIA","photoUrl":"","userId":"01583325167782597357"}},"outputId":"5c1e635f-d7d1-4533-8733-e418f441dff4","colab":{"base_uri":"https://localhost:8080/","height":521}},"source":["df=pd.DataFrame(numbers,columns=['category','sentiment','dataset','number'])\n","df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>category</th>\n","      <th>sentiment</th>\n","      <th>dataset</th>\n","      <th>number</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>dvd</td>\n","      <td>positive</td>\n","      <td>train</td>\n","      <td>800</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>dvd</td>\n","      <td>positive</td>\n","      <td>test</td>\n","      <td>200</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>dvd</td>\n","      <td>negative</td>\n","      <td>train</td>\n","      <td>800</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>dvd</td>\n","      <td>negative</td>\n","      <td>test</td>\n","      <td>200</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>kitchen</td>\n","      <td>positive</td>\n","      <td>train</td>\n","      <td>800</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>kitchen</td>\n","      <td>positive</td>\n","      <td>test</td>\n","      <td>200</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>kitchen</td>\n","      <td>negative</td>\n","      <td>train</td>\n","      <td>800</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>kitchen</td>\n","      <td>negative</td>\n","      <td>test</td>\n","      <td>200</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>electronics</td>\n","      <td>positive</td>\n","      <td>train</td>\n","      <td>800</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>electronics</td>\n","      <td>positive</td>\n","      <td>test</td>\n","      <td>200</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>electronics</td>\n","      <td>negative</td>\n","      <td>train</td>\n","      <td>800</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>electronics</td>\n","      <td>negative</td>\n","      <td>test</td>\n","      <td>200</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>book</td>\n","      <td>positive</td>\n","      <td>train</td>\n","      <td>800</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>book</td>\n","      <td>positive</td>\n","      <td>test</td>\n","      <td>200</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>book</td>\n","      <td>negative</td>\n","      <td>train</td>\n","      <td>800</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>book</td>\n","      <td>negative</td>\n","      <td>test</td>\n","      <td>200</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       category sentiment dataset  number\n","0           dvd  positive   train     800\n","1           dvd  positive    test     200\n","2           dvd  negative   train     800\n","3           dvd  negative    test     200\n","4       kitchen  positive   train     800\n","5       kitchen  positive    test     200\n","6       kitchen  negative   train     800\n","7       kitchen  negative    test     200\n","8   electronics  positive   train     800\n","9   electronics  positive    test     200\n","10  electronics  negative   train     800\n","11  electronics  negative    test     200\n","12         book  positive   train     800\n","13         book  positive    test     200\n","14         book  negative   train     800\n","15         book  negative    test     200"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"r9gyyJxZ9959"},"source":["Training data for books:\n","\n","\n","1.   positive = 800\n","2.   negative = 800\n","\n","Negative testing data:\n","  200 of each\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FTGHJWSd7iQ5"},"source":["## Creating word lists\n","The next section will explain how to use a sentiment classifier that bases its decisions on word lists. The classifier requires a list of words indicating positive sentiment, and a second list of words indicating negative sentiment. Given positive and negative word lists, a document's overall sentiment is determined based on counts of occurrences of words that occur in the two lists. In this section we are concerned with the creation of the word lists. We will be considering both hand-crafted lists and automatically generated lists."]},{"cell_type":"markdown","metadata":{"id":"572x5pEP7iQ6"},"source":["### Exercise 2.1\n","\n","- Create a reasonably long hand-crafted list of words that you think indicate positive sentiment.\n","- Create a reasonably long hand-crafted list of words that indicate negative sentiment.\n","\n","Use the following cells to store these lists in the variables `my_positive_word_list` and `my_negative_word_list`."]},{"cell_type":"code","metadata":{"id":"RPzluDd-7iQ6"},"source":["my_positive_word_list = [\"good\",\"great\",\"lovely\", \"excellent\"] # extend this one or put your own list here\n","my_negative_word_list = [\"bad\", \"terrible\", \"awful\", \"dreadful\"] # extend this one or put your own list here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n-Y7uHzv7iQ-"},"source":["Next, you should try to derive word lists from the data. One way to do this, is to use the most frequent words in positive reviews as your positive list, and the most frequent words in negative reviews as your negative list. This can be done with the [NLTK <code style=\"background-color: #F5F5F5;\">FreqDist</code>](http://www.nltk.org/api/nltk.html#module-nltk.probability) object. \n","\n","> You should make sure you understand the code in the cell below."]},{"cell_type":"code","metadata":{"id":"htdnS9087iQ_"},"source":["from nltk.probability import FreqDist # see http://www.nltk.org/api/nltk.html#module-nltk.probability\n","from sussex_nltk.corpus_readers import AmazonReviewCorpusReader\n","from functools import reduce # see https://docs.python.org/3/library/functools.html\n","\n","#Helper function. Given a list of reviews, return a list of all the words in those reviews\n","#To understand this look at the description of functools.reduce in https://docs.python.org/3/library/functools.html\n","def get_all_words(amazon_reviews):\n","\n","    return reduce(lambda words,review: words + review.words(), amazon_reviews, [])\n","\n","#A frequency distribution over all words in positive book reviews\n","pos_freqdist = FreqDist(get_all_words(dvd_pos_train))\n","neg_freqdist = FreqDist(get_all_words(dvd_neg_train))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nGSZAc10HEJf"},"source":["Some more examples of reduce"]},{"cell_type":"code","metadata":{"id":"AfqQ4n4EG8e2","executionInfo":{"status":"ok","timestamp":1603286933750,"user_tz":-60,"elapsed":4721,"user":{"displayName":"ZHENYI JIA","photoUrl":"","userId":"01583325167782597357"}},"outputId":"d99a5193-b4b9-4459-8e28-86ed2c26e896","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#adding up a list\n","mylist=[1,4,5,2,98]\n","reduce(lambda x,y:x+y,mylist,0)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["110"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"69kOnMnEHjho","executionInfo":{"status":"ok","timestamp":1603184639626,"user_tz":-60,"elapsed":648,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"28408697-5d01-4aa7-ce01-e15c11989b9d","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["mylist=['t','h','e',' ','d','o','g']\n","reduce(lambda x,y:x+y,mylist,\"\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'the dog'"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"n1Xoz1g3Io_Y"},"source":["So what does get_allwords() do?"]},{"cell_type":"code","metadata":{"id":"GSOFm8N8IsXS","executionInfo":{"status":"ok","timestamp":1603184673338,"user_tz":-60,"elapsed":622,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"36b7da39-2ea5-4049-f3dc-0d5c892230e6","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["for review in dvd_pos_train:\n","  print(review.words())\n","  break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['This', 'wonderful', 'TV', 'special', 'from', 'the', '70', \"'s\", 'is', 'timeless', 'and', 'has', 'great', 'music', 'by', 'Harry', 'Nihlssen', '(', 'not', 'sure', 'of', 'that', 'spelling', ')', '.', 'The', 'lesson', 'is', 'all', 'about', 'tolerance', 'with', 'lots', 'of', 'side', 'issues', 'of', 'great', 'value', '.', 'I', 'am', 'thrilled', 'to', 'own', 'this', 'DVD', '.', 'It', 'brought', 'back', 'all', 'kinds', 'of', 'memories', 'for', 'me', '-', 'both', 'the', 'story', 'AND', 'especially', 'the', 'music', '.', 'It', 'is', 'a', 'perfect', 'addition', 'to', 'my', 'collection', 'of', 'movies', 'for', 'my', 'grandchildren']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"13ChATcII-ur"},"source":["Its just going to make a long (flattened) list of all of the words in the reviews.\n","\n","The constructor for FreqDist will count up how many there are of each type and store this in an object which is very similar to a dictionary.  Its just specialised so that the keys are strings and the values are counts.   We can look things up in a FreqDist in exactly the same way as in a dictionary.  But it supports some other operations too (such as adding and subtraction)"]},{"cell_type":"code","metadata":{"id":"D0LqzCRC7iRB"},"source":["pos_freqdist"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nhF3ZvUaQozI","executionInfo":{"status":"ok","timestamp":1603184801038,"user_tz":-60,"elapsed":853,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"bb3d0d32-6b59-4558-d7a2-f27ab658faf2","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["pos_freqdist['This']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["398"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"tOth3UgpQr37","executionInfo":{"status":"ok","timestamp":1603184816607,"user_tz":-60,"elapsed":940,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"3afd4df6-d705-4f39-c321-2bdcca9de908","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["pos_freqdist['this']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1319"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"sDPYLthz7iRG"},"source":["### Exercise 2.2\n","Explain (in words) how the <code>get_all_words()</code> function works.  Your description should include details about\n","1. the input\n","2. the output\n","3. the algorithm used to generate the output from the input"]},{"cell_type":"markdown","metadata":{"id":"VplLv2_A7iRG"},"source":["YOU NEED TO TYPE SOME WORDS HERE!"]},{"cell_type":"markdown","metadata":{"id":"RW9BMqP37iRH"},"source":["### Exercise 2.3\n","In the blank code cell below write code that uses the frequency lists, `pos_freqdist` and `neg_freqdist`, created in the above cell and `my_positive_word_list` and `my_negative_word_list` that you manually created earlier to determine whether or not the review data conforms to your expectations. In particular, whether:\n","- the words you expected to indicate positive sentiment actually occur more frequently in positive reviews than negative reviews\n","- the words you expected to indicate negative sentiment actually occur more frequently in negative reviews than positive reviews.\n","\n","Display your findings in a table using pandas."]},{"cell_type":"code","metadata":{"id":"JPRJHuET7iRH"},"source":["def check_expectations(a_word_list,expectation,pos_freqdist=pos_freqdist,neg_freqdist=neg_freqdist):\n","#expectation is a positive number if words are expected to be positive\n","#expectation is a negative number if words are expected to be negative\n","\n","    for word in a_word_list:\n","        pos_freq=pos_freqdist.get(word,0)\n","        neg_freq=neg_freqdist.get(word,0)\n","        diff=pos_freq-neg_freq\n","        if diff*expectation>0:\n","            print(\"As expected: for {} difference is {}\".format(word,diff))\n","        else:\n","            print(\"Contrary to expectations: for {} difference is {}\".format(word,diff))\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b4aR84LE7iRL","executionInfo":{"status":"ok","timestamp":1603184989054,"user_tz":-60,"elapsed":599,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"05ae05af-f7f7-4dd9-ff5d-c9760c3f7d8e","colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["check_expectations(my_positive_word_list,1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Contrary to expectations: for good difference is -41\n","As expected: for great difference is 165\n","As expected: for lovely difference is 5\n","As expected: for excellent difference is 51\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bkfwFU4v7iRO","executionInfo":{"status":"ok","timestamp":1603185010959,"user_tz":-60,"elapsed":971,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"daef84ba-b3a0-4899-9a6e-6bdb7fcce2fc","colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["check_expectations(my_negative_word_list,-1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["As expected: for bad difference is -140\n","As expected: for terrible difference is -34\n","As expected: for awful difference is -37\n","As expected: for dreadful difference is -2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8bXExCJp7iRS"},"source":["### Exercise 2.4\n","Now, you are going to create positive and negative word lists automatically from the training data. In order to do this:\n","\n","1. write two new functions to help with automating the process of generating wordlists.\n","\n","    - `most_frequent_words` - this function should take THREE arguments: 2 frequency distributions and a natural number, k. It should order words by how much more they occur in one frequency distribution than the other.   It should then return the top k highest scoring words. You might want to use the `most_common` method from the `FreqDist` class - this returns a list of word, frequency pairs ordered by frequency.  You might also or alternatively want to use pythons built-in `sorted` function\n","    - `words_above_threshold` - this function also takes three arguments: 2 frequency distributions and a natural number, k. Again, it should order words by how much more they occur in one distribution than the other.  It should return all of the words that have a score greater than k.\n","\n","2. Remove punctuation and stopwords from consideration. You can re-use code from near the end of Lab_2_2.\n","3. Using the training data, create two sets of positive and negative word lists using these functions (1 set with each function). \n","4.  Display these 4 lists (possibly in a `Pandas` dataframe?)\n","\n"]},{"cell_type":"code","metadata":{"id":"GUxe8Ol07iRS"},"source":["posdiff=pos_freqdist-neg_freqdist\n","posdiff"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6KwdjbI-7iRW","executionInfo":{"status":"ok","timestamp":1603185117181,"user_tz":-60,"elapsed":592,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"f21c0274-bcbb-4645-d848-cb8ce0a8b4e0","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["posdiff.get('excellent',0)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["51"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"0boOFfX8_DkE","executionInfo":{"status":"ok","timestamp":1603185135274,"user_tz":-60,"elapsed":961,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"1cb95112-1e6f-408b-e306-94ebda943464","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["posdiff.get('good',0)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"2ma9fpwGSQyz"},"source":["pos_freqdist.most_common()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o6hw9SFM7iRY"},"source":["from nltk.corpus import stopwords\n","stop = stopwords.words('english')\n","\n","def most_frequent_words(posfreq,negfreq,topk):\n","    difference=[(w,f-negfreq.get(w,0)) for (w,f) in posfreq.most_common()]\n","    sorteddiff=sorted(difference,key=lambda pair:pair[1],reverse=True)\n","    normalised=[w.lower() for (w,f) in sorteddiff]\n","    filtered=[w for w in normalised if w.isalpha() and w not in stop]\n","    return filtered[:topk]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jtwxYJmQ7iRb","executionInfo":{"status":"ok","timestamp":1603287328202,"user_tz":-60,"elapsed":1633,"user":{"displayName":"ZHENYI JIA","photoUrl":"","userId":"01583325167782597357"}},"outputId":"6f3fb7dc-6d5e-4e25-eadb-e294fde32e83","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["top_pos=most_frequent_words(pos_freqdist,neg_freqdist,50)\n","print(top_pos)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['great', 'well', 'also', 'best', 'film', 'love', 'still', 'first', 'family', 'many', 'wonderful', 'see', 'always', 'excellent', 'classic', 'gives', 'show', 'music', 'one', 'enjoy', 'set', 'story', 'comedy', 'must', 'lot', 'john', 'episode', 'season', 'loved', 'perfect', 'years', 'performance', 'men', 'man', 'us', 'fun', 'collection', 'young', 'hilarious', 'watch', 'shows', 'films', 'series', 'amazing', 'day', 'played', 'performances', 'enjoyed', 'old', 'one']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kP84olqo7iRf","executionInfo":{"status":"ok","timestamp":1603185631449,"user_tz":-60,"elapsed":820,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"a348e11c-4b0d-4395-db44-95412e157083","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["top_neg=most_frequent_words(neg_freqdist,pos_freqdist,50)\n","print(top_neg)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['movie', 'like', 'bad', 'would', 'could', 'worst', 'better', 'even', 'nothing', 'acting', 'boring', 'waste', 'book', 'money', 'much', 'character', 'movies', 'make', 'plot', 'minutes', 'instead', 'horrible', 'want', 'people', 'stupid', 'something', 'going', 'good', 'scenes', 'quality', 'script', 'awful', 'erin', 'supposed', 'worse', 'problem', 'either', 'terrible', 'get', 'think', 'version', 'read', 'actors', 'dialogue', 'way', 'say', 'least', 'away', 'felt', 'ridiculous']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ABq5Sb0j_j2p"},"source":["def above_threshold(posfreq,negfreq,threshold):\n","  difference=[(w.lower(),f-negfreq.get(w,0)) for (w,f) in posfreq.most_common()]\n","  sorteddiff=sorted(difference,key=lambda pair:pair[1],reverse=True)\n","  filtered=[w for (w,f) in sorteddiff if w.isalpha() and w not in stop and f>threshold]\n","  return filtered"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"idr8XYkWAmfl","executionInfo":{"status":"ok","timestamp":1603185659293,"user_tz":-60,"elapsed":594,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"3d17e3d9-0fa5-4aac-940e-7d1ee39c410d","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["above100pos = above_threshold(pos_freqdist,neg_freqdist,20)\n","print(above100pos)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['great', 'well', 'also', 'best', 'love', 'one', 'family', 'still', 'first', 'many', 'episode', 'show', 'wonderful', 'always', 'season', 'excellent', 'gives', 'music', 'perfect', 'fun', 'man', 'enjoy', 'classic', 'john', 'loved', 'watch', 'collection', 'enjoyed', 'hilarious', 'film', 'times', 'lot', 'one', 'workout', 'years', 'old', 'episodes', 'men', 'frank', 'especially', 'life', 'young', 'true', 'anyone', 'favorite', 'amazing', 'musical', 'best', 'never', 'makes', 'must', 'vs', 'little', 'comedy', 'job', 'day', 'role', 'war', 'body', 'includes', 'adrian', 'series', 'cast', 'shows', 'wife', 'plays', 'son', 'woman', 'takes', 'documentary', 'great', 'named', 'bergman', 'russell', 'lily', 'different', 'along', 'fine', 'view', 'course', 'friend', 'features', 'jack', 'city', 'see', 'work', 'set', 'though', 'performances', 'able', 'including', 'finds', 'terrific', 'gehry']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FaqsrbFTAmwI","executionInfo":{"status":"ok","timestamp":1603185663260,"user_tz":-60,"elapsed":803,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"2e2bfa97-4e0e-4fee-c782-f3ec149a9407","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["above100neg = above_threshold(neg_freqdist,pos_freqdist,100)\n","print(above100neg)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['movie', 'like', 'bad', 'would', 'could']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xzVP-hJH7iRi"},"source":["## Creating a word list based classifier\n","Now you have a number of word lists for use with a classifier. \n","> Make sure you understand the following code, which will be used as the basis for creating a word list based classifier."]},{"cell_type":"code","metadata":{"id":"CSaLWU_A7iRi"},"source":["from nltk.classify.api import ClassifierI\n","import random\n","\n","class SimpleClassifier(ClassifierI): \n","\n","    def __init__(self, pos, neg): \n","        self._pos = pos \n","        self._neg = neg \n","\n","    def classify(self, words): \n","        score = 0\n","        \n","        # add code here that assigns an appropriate value to score\n","        return \"N\" if score < 0 else \"P\"\n","\n","    def batch_classify(self, docs): \n","        return [self.classify(doc.words() if hasattr(doc, 'words') else doc) for doc in docs] \n","\n","    def labels(self): \n","        return (\"P\", \"N\")\n","\n","#Example usage:\n","\n","classifier = SimpleClassifier(top_pos, top_neg)\n","classifier.classify(\"I read the book\".split())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"irq5PVOc7iRl"},"source":["### Exercise 3.1\n","\n","- Copy the above code cell and move it to below this one. Then complete the `classify` method in the above code as specified below.\n","- Test your classifier on several very simple hand-crafted examples to verify that you have implemented `classify` correctly.\n","\n","The classifier is initialised with a list of positive words, and a list of negative words. The words of a document are passed to the `classify` method (which is partially completed in the above code fragment). The `classify` method should be defined so that each occurrence of a negative word decrements `score`, and each occurrence of a positive word increments `score`. \n","- For `score` less than 0, an \"`N`\" for negative should be returned.\n","- For `score` greater than 0,  \"`P`\" for positive should returned.\n","- For `score` of 0, the classification decision should be made randomly (see https://docs.python.org/3/library/random.html).\n"]},{"cell_type":"code","metadata":{"id":"0UXUyFHM7iRm","executionInfo":{"status":"ok","timestamp":1603185969423,"user_tz":-60,"elapsed":745,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"ebff76da-57a5-42ac-fcc8-e08a6e23595f","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["\n","from nltk.classify.api import ClassifierI\n","import random\n","\n","class SimpleClassifier(ClassifierI): \n","\n","    def __init__(self, pos, neg): \n","        self._pos = pos \n","        self._neg = neg \n","\n","    def classify(self, words): \n","        score = 0\n","        \n","        # add code here that assigns an appropriate value to score\n","        for word in words:\n","            if word in self._pos:\n","                score+=1\n","            if word in self._neg:\n","                score-=1\n","        \n","        return \"N\" if score < 0 else \"P\" \n","\n","    def batch_classify(self, docs): \n","        return [self.classify(doc.words() if hasattr(doc, 'words') else doc) for doc in docs] \n","\n","    def labels(self): \n","        return (\"P\", \"N\")\n","\n","#Example usage:\n","\n","classifier = SimpleClassifier(top_pos, top_neg)\n","classifier.classify(\"I hated this awful movie\".split())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'N'"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"GRJWPhUF7iRo"},"source":["### Exercise 3.2\n","* Extend your SimpleClassifier class so that it has a `train` function which will derive the wordlists from training data.  You could build a separate class for each way of automatically deriving wordlists (which both inherit from SimpleClassifier) OR a single class which takes an extra parameter at training time."]},{"cell_type":"code","metadata":{"id":"KT1PbIao7iRp"},"source":["class SimpleClassifier_mf(SimpleClassifier):\n","    \n","    def __init__(self,k):\n","        self._k=k\n","    \n","    def train(self,pos_train,neg_train):\n","        pos_freqdist = FreqDist(get_all_words(pos_train))\n","        neg_freqdist = FreqDist(get_all_words(neg_train))\n","        self._pos=most_frequent_words(pos_freqdist,neg_freqdist,self._k)\n","        self._neg=most_frequent_words(neg_freqdist,pos_freqdist,self._k)\n","    \n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h85qqrOV7iRr"},"source":["dvdclassifier=SimpleClassifier_mf(100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3uM8En0_7iRu"},"source":["dvdclassifier.train(dvd_pos_train,dvd_neg_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1fWoIb8dDYzR"},"source":["Try out your classifier on the test data.  We will look at how to evaluate classifiers next week, but in an ideal world, most of the positive test items will have been classified as 'P' and most of the negative test items will have been classified as 'N' "]},{"cell_type":"code","metadata":{"id":"s6fQs92SVu39","executionInfo":{"status":"ok","timestamp":1603186219786,"user_tz":-60,"elapsed":566,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"0843ec0b-31b2-44e9-e8d0-1c9e898187c7","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["dvdclassifier.classify(\"I hated this movie\".split())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'N'"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"code","metadata":{"id":"vFCEF0rx7iRx"},"source":["dvdclassifier.batch_classify(dvd_pos_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q1dICMaw7iR4","executionInfo":{"status":"ok","timestamp":1603186265329,"user_tz":-60,"elapsed":883,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"6896ba87-6e79-4a7d-ad55-70efc427f592","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["dvdclassifier.batch_classify(dvd_neg_test)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'P',\n"," 'P',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'P',\n"," 'P',\n"," 'N',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'P',\n"," 'P',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'P',\n"," 'P',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'P',\n"," 'P',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'P',\n"," 'P',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'P',\n"," 'P',\n"," 'P',\n"," 'P',\n"," 'N',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'P',\n"," 'P',\n"," 'N',\n"," 'P',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N',\n"," 'N']"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"code","metadata":{"id":"i9MTBKOuDPU9"},"source":[""],"execution_count":null,"outputs":[]}]}