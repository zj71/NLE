{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Lab_8_1_solutions.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"CNr7m4n3Fvu7"},"source":["# Week 8: Part-of-Speech Tagging \n","\n","This week we are learn about part-of-speech (POS) tagging.  This involves deciding the correct part-of speech tag (e.g., noun, verb, adjective etc) for each word in a sentence.  Since the correct tag for each word depends not only on the current word but on the tags of those words around it, it is generally viewed as a **sequence labelling** problem.  In other words, for a given sequence of words, we are asking what is the most likely sequence of tags?\n"]},{"cell_type":"code","metadata":{"id":"avA_7Rf2Fvu9"},"source":["#mount google drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","import sys\n","import operator\n","#make sure you append the path where your utils.py file is.\n","sys.path.append('/content/drive/My Drive/NLE Notebooks/Week4LabsSolutions/')\n","from utils import *\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UyHtJ98mFvvD"},"source":["## Average PoS tag ambiguity \n","The Part-of-Speech (PoS) tag ambiguity of a word type is a measure of how varied the PoS tags are for that type.   Note that here, we talk about the ambiguity of a word type rather than a word token because any given token has a single tag but different occurrences of the same type may have different tags.  For example, some occurrences of the word *bank* have the tag *noun* whereas others have the tag *verb*\n","\n","Some types are always (or almost always) labelled with the same PoS tag, so exhibit no (or very little) ambiguity. It is easy to predict the correct PoS tag for such words. \n","\n","On the other hand, a type that is commonly labelled by a variety of different PoS tags exhibits a high level of ambiguity, and is more challenging to deal with.\n","\n","In this session, we are going to be considering two measures of a type's ambiguity. We will be using the Wall Street Journal corpus as it has been hand-annotated with part of speech tags. \n","We will consider \n","* a simple measure that just **counts** the number of different tags that label the type. \n","* a more complex information-theoretic measure based on **entropy**.\n","\n","First, we create an instance of a `WSJCorpusReader`.  Then we can use the method `tagged_words()` to get a list of all tokens in the corpus tagged with their POS."]},{"cell_type":"code","metadata":{"id":"jBrm8CrgFvvE"},"source":["from sussex_nltk.corpus_readers import WSJCorpusReader\n","\n","wsjreader=WSJCorpusReader()\n","taggedWSJ=wsjreader.tagged_words()\n","for i,(token,tag) in enumerate(taggedWSJ):\n","    print(i,token,tag)\n","    if i>10:\n","        break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yeHgtOFrFvvT"},"source":["### Exercise 1.1\n","Write a function `find_tag_distributions(tokentaglist)` which finds the (frequency) distributions of tags for every word in its input.\n","* input: a list of pairs (token,tag)\n","* returns: a dictionary of dictionaries.  The key to the outermost dictionary should be the word and the key to each internal dictionary should be the tag.  The value associated with the tag in the internal dictionary should be its frequency of occurrence.\n","\n","Note that this exercise is very similar to Ex1.1 in Lab_7_1\n","\n","Test your function on `taggedWSJ` and look at the tag distribution for the word `bank`.  Note that because taggedWSJ is a generator, you will need to reinstantiate it by creating a new WSJReader.   You should find that you get:\n","\n","`{NN: 521,\n","VB: 1,\n","VBP: 1}`\n","\n"]},{"cell_type":"code","metadata":{"id":"GIuDtQHOFvvU"},"source":["def find_tag_distributions(tokentaglist,num=-1):\n","    tag_dists={}\n","    for i,(token,tag) in enumerate(tokentaglist):\n","      current=tag_dists.get(token,{})\n","      current[tag]=current.get(tag,0)+1\n","      tag_dists[token]=current\n","      if num > 0 and i>num:\n","        print(\"Max number exceeded\")\n","        break\n","    return tag_dists\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MbT0LK_jFvvZ"},"source":["import time\n","starttime=time.time()\n","wsjreader=WSJCorpusReader()\n","taggedWSJ=wsjreader.tagged_words()\n","distsWSJ=find_tag_distributions(taggedWSJ,num=400000)\n","timetaken=time.time()-starttime\n","print(\"Time taken {}\".format(timetaken))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Gj0dZW1SXaY"},"source":["import time\n","starttime=time.time()\n","wsjreader=WSJCorpusReader()\n","taggedWSJ=wsjreader.tagged_words()\n","taggedWSJlist=list(taggedWSJ)\n","timetaken=time.time()-starttime\n","print(\"Time taken {}\".format(timetaken))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"flhCEOXoLrEN"},"source":["distsWSJ['the']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hfNPiMo9M9T3"},"source":["distsWSJ['bank']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uuJxTum4Fvve"},"source":["distsWSJ['dream']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ITszNE8Fvvi"},"source":["distsWSJ['every']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PgWSrwOOFvvl"},"source":["distsWSJ['night']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OJYn8xEDFvvp"},"source":["distsWSJ['I']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XrNX24wsFvvs"},"source":["distsWSJ['same']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qq-OqloiFvvv"},"source":["distsWSJ['like']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4JX0acXFVsBG"},"source":["distsWSJ['I']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QArRz4NYFvvy"},"source":["### Exercise 1.2\n","Write a function `simple_pos_ambiguity` which can take the tagged WSJ text and returns a dictionary containing the number of part of speech tags which each word type has.  Note that this is simply the length of the dictionary associated with that word in the output from `find_tag_distributions`.\n","\n","Check that you get the following results:\n","bank: 3\n","blue: 2\n","walk: 3"]},{"cell_type":"code","metadata":{"id":"5wj-OMMgFvvz"},"source":["def simple_pos_ambiguity(tag_dists):\n","  return {word:len(tag_dist.keys()) for word,tag_dist in tag_dists.items()}\n","\n","#wsjreader=WSJCorpusReader()\n","#taggedWSJ=wsjreader.tagged_words()\n","ambiguity=simple_pos_ambiguity(distsWSJ)\n","words=['bank','blue','walk','show']\n","for word in words:\n","    print(\"{}: {}\".format(word,ambiguity[word]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TSUNSo13Fvv2"},"source":["### Exercise 1.3\n","Find the mean average value of the `simple_pos_ambiguity` score for word types in the WSJ."]},{"cell_type":"code","metadata":{"id":"6kdfVMgBFvv2"},"source":["import numpy as np\n","print(np.mean(list(ambiguity.values())))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"em4JvkMTFvv5"},"source":["## Entropy as a Measure of Tag Ambiguity\n","\n","**Entropy** is a measure of uncertainty. A word will have high entropy when it occurs the same number of times with each part of speech. There is maximum uncertainty as to which part of speech it has.\n","\n","The larger the part of speech tagset, the greater the potential for uncertainty, and the higher the entropy can be.\n","\n","In the cell below we see a function `entropy`. It's argument is a list of counts (which in our case are counts of how many times a word appeared with a given part of speech).\n","\n","Check that you understand how the code implements this definition of entropy:\n","$$H([x_1,\\ldots,x_n])= - \\sum_{i=1}^nP(x_i)\\log_2 P(x_i)$$\n","where $n$ is the number of PoS tags, and $x_i$ is a count of how many times the word was labelled with the $i$th PoS tag."]},{"cell_type":"code","metadata":{"id":"fl2bLOJjFvv5","executionInfo":{"status":"ok","timestamp":1610801109114,"user_tz":0,"elapsed":1078,"user":{"displayName":"ZHENYI JIA","photoUrl":"","userId":"01583325167782597357"}}},"source":["def entropy(counts):            # counts = list of counts of occurrences of tags\n","    total = sum(counts)         # get total number of occurrences\n","    if not total: return 0      # if zero occurrences in total, then 0 entropy\n","    entropy = 0\n","    for i in counts:            # for each tag count\n","        p = i/total      # probability that the token occurs with this tag\n","        try:\n","            entropy += p * math.log(p,2) # add to entropy\n","        except ValueError: pass     # if p==0, then ignore this p\n","    return -entropy if entropy else entropy   # only negate if nonzero, otherwise \n","                                              # floats can return -0.0, which is weird.\n"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yZUPeuVuFvv9"},"source":["### Exercise 2.1\n","Experiment with the `entropy` function.\n","- It takes a list of counts as its argument.\n","- Compare the entropy of a list where all counts are the same with the entropy of a list of different counts.\n","- Investigate the effect of varying the length of the list of counts."]},{"cell_type":"code","metadata":{"id":"Z4Lb79_qFvv9","colab":{"base_uri":"https://localhost:8080/","height":282},"executionInfo":{"status":"error","timestamp":1610801119824,"user_tz":0,"elapsed":621,"user":{"displayName":"ZHENYI JIA","photoUrl":"","userId":"01583325167782597357"}},"outputId":"2d1aca6b-5d43-4219-e9b2-d67ed33740e0"},"source":["entropy([0.8,0.2])"],"execution_count":3,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-f29ff1e7bc64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-d7b31262015b>\u001b[0m in \u001b[0;36mentropy\u001b[0;34m(counts)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal\u001b[0m      \u001b[0;31m# probability that the token occurs with this tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mentropy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# add to entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mpass\u001b[0m     \u001b[0;31m# if p==0, then ignore this p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mentropy\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentropy\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mentropy\u001b[0m   \u001b[0;31m# only negate if nonzero, otherwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'math' is not defined"]}]},{"cell_type":"code","metadata":{"id":"4xatJmrvU5WH"},"source":["entropy([100000,10,10,10,10])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jgjNBjOvFvwA"},"source":["### Exercise 2.2\n","Write a function `entropy_ambiguity` which takes the tagged WSJ text and returns a dictionary containing the entropy of each word.\n","\n","Test it out your function; you should find:\n","\n","`bank: 0.04004053596567404\n","blue: 0.4394969869215134\n","walk: 1.3127443531093745\n","show: 1.5322594002899546`\n","\n","How does this correspond to our intuitions about which word types are more difficult to correctly POS tag?"]},{"cell_type":"code","metadata":{"id":"Vv7DgTJmFvwB"},"source":["def entropy_ambiguity(tag_dists):\n","    #tag_dists=find_tag_distributions(tokentaglist)\n","    ent_ambiguity={key:entropy(dist.values()) for key,dist in tag_dists.items()}\n","    return ent_ambiguity\n","\n","#wsjreader=WSJCorpusReader()\n","#taggedWSJ=wsjreader.tagged_words()\n","entropydict=entropy_ambiguity(distsWSJ)\n","words=['bank','blue','walk','show','the', 'The']\n","words+=\"every night I dream the same\".split()\n","for word in words:\n","    print(\"{}: {}\".format(word,entropydict.get(word,0)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5CYuAcF-WDcq"},"source":["distsWSJ['the']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"skoLCQTUWG-c"},"source":["distsWSJ['The']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PdE0Tk3dFvwE"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"yVla9DKcFvwF"},"source":["## A Simple Unigram Tagger\n","Now, we will be looking at part of speech tagging itself i.e., the problem of determining the correct tag for a given word token. We will\n","\n","* implement a unigram tagger\n","* experiment with an off-the-shelf POS tagger which utilises information about the previous words or tags in the sequence.\n","\n","First, lets get some tagged text from the WSJ and split it into a training and a testing set."]},{"cell_type":"code","metadata":{"id":"o7QD05jqFvwF"},"source":["def get_train_test_pos(split=0.7):\n","\n","    from sussex_nltk.corpus_readers import WSJCorpusReader\n","    wsjreader=WSJCorpusReader()\n","    taggedWSJ=wsjreader.tagged_words()\n","    taggedlist=list(taggedWSJ)\n","    \n","    #we don't want to randomly select data because we need to preserve sequence information\n","    #so we are just going to take the first part as training and the second as test\n","    n=int(len(taggedlist)*split)\n","    return taggedlist[:n],taggedlist[n:]\n","\n","train, test = get_train_test_pos(split=0.8)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ycHhfUHwXGLq"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nk1z9qGbFvwI"},"source":["Now, we build a unigram model of the tag distribution for each word type.  We use the `find_tag_distributions` function defined earlier and store the result in the variable `unigram_model`"]},{"cell_type":"code","metadata":{"id":"nHPFWWjBFvwJ"},"source":["unigram_model=find_tag_distributions(train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mTJHn-YZXeTr"},"source":["unigram_model.get('the',{})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DVrnzCnTFvwM"},"source":["### Exercise 3.1\n","Write a `uni_pos_tag` function which takes:\n","* a sequence of tokens \\[wordtoken1,wordtoken2, ....\\]\n","* a unigram model (stored as a dictionary of dictionaries\n","and returns:\n","* a tagged sequence of tokens \\[(wordtoken1,tag1),(wordtoken2,tag2),....\\]\n","\n"]},{"cell_type":"code","metadata":{"id":"3SIfLgGUFvwN"},"source":["def best_tag(word,unimodel):\n","    dist=unimodel.get(word,{})\n","    ordered=sorted(list(dist.items()),key=operator.itemgetter(1),reverse=True)\n","    return ordered[0][0]\n","def uni_pos_tag(words,unimodel):\n","    return [(word,best_tag(word,unimodel)) for word in words]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I6qzxKZ4FvwQ"},"source":["### Exercise 3.2\n","Test that your function works on both the training data `train` and the testing data `test`.  Remember, you can separate the tokens and the tags into two separate lists using:\n","* `train_toks,train_tags=zip(*train)`\n","* `test_toks,test_tags=zip(*test)`\n","\n","Don't worry about evaluating the accuracy at this point (that's the next exercise) - just check that you can generate sequences of (token,tag) pairs in both cases.  What happens if there is a word in the test data that didn't occur in the training data?  You might need to update your `uni_pos_tag` function to take this into account."]},{"cell_type":"code","metadata":{"id":"q9D6cNoDFvwQ"},"source":["train_toks,train_tags=zip(*train)\n","uni_pos_tag(train_toks,unigram_model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UEcDUP6kFvwT"},"source":["test_toks,test_tags=zip(*test)\n","uni_pos_tag(test_toks,unigram_model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Vep-r8lFvwW"},"source":["def back_off(unimodel):\n","    #find which is the most frequent tag assigned to any type\n","    combined={}\n","    for adict in unimodel.values():\n","        for (tag,value) in adict.items():\n","            combined[tag]=combined.get(tag,0)+value\n","    ordered=sorted(list(combined.items()),key=operator.itemgetter(1),reverse=True)\n","    return ordered[0][0]\n","\n","back_off(unigram_model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xev9H6blFvwY"},"source":["def best_tag(word,unimodel,default='N'):\n","    dist=unimodel.get(word,{default:1})\n","    ordered=sorted(list(dist.items()),key=operator.itemgetter(1),reverse=True)\n","    return ordered[0][0]\n","def uni_pos_tag(words,unimodel):\n","    default_tag=back_off(unimodel)\n","    return [(word,best_tag(word,unimodel,default=default_tag)) for word in words]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tF6Y9JiRFvwc"},"source":["test_toks,test_tags=zip(*test)\n","uni_pos_tag(test_toks,unigram_model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_jTQsUrlFvwe"},"source":["### Exercise 3.3\n","Write a function `evaluate_uni_pos_tag` which will calculate the accuracy of the `uni_pos_tag` function. This should have as arguments:\n","* the unigram_model\n","* the gold standard sequence of (token,tag) pairs for comparison\n","\n","You should find that it is 94.6% accurate on the training data.  How accurate is it on the test data? \n","\n","As an extension, you could implement a uni_pos_tagger class, which combines the all of the functionality above, and then provide an `evaluate` function which evaluates a tagger. \n"]},{"cell_type":"code","metadata":{"id":"OFmQ_ZgjFvwf","executionInfo":{"status":"ok","timestamp":1610803387825,"user_tz":0,"elapsed":537,"user":{"displayName":"ZHENYI JIA","photoUrl":"","userId":"01583325167782597357"}}},"source":["def evaluate_uni_pos_tag(unigram_model,goldstandard):\n","    goldtoks,goldtags=zip(*goldstandard)\n","    pretoks,pretags=zip(*uni_pos_tag(goldtoks,unigram_model))\n","    print(goldtags[:10])\n","    print(pretags[:10])\n","    n=len(pretags)\n","    correct=0\n","    for (pre,gold) in zip(pretags,goldtags):\n","        if pre==gold:\n","            correct+=1\n","    return correct/n\n","\n","    "],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"hnAThHYEFvwh","colab":{"base_uri":"https://localhost:8080/","height":165},"executionInfo":{"status":"error","timestamp":1610803391300,"user_tz":0,"elapsed":709,"user":{"displayName":"ZHENYI JIA","photoUrl":"","userId":"01583325167782597357"}},"outputId":"8efd13c8-b58a-44c5-cd4c-8b8cc1f1536b"},"source":["evaluate_uni_pos_tag(unigram_model,train)"],"execution_count":6,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-44f21c7dd5d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_uni_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munigram_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'unigram_model' is not defined"]}]},{"cell_type":"code","metadata":{"id":"olWTComLFvwk"},"source":["evaluate_uni_pos_tag(unigram_model,test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"85cP6GboFvwm"},"source":["## Beyond Unigram Tagging\n","State-of-the-art POS-taggers use information about likely sequences of tags to get higher performance.\n","\n","The `pos_tag` function provided by nltk uses a perceptron tagger, the technicalities of which are beyond the scope of this course.  However, we can run it and evaluate it on our sequences of tokens in the same way as our `uni_pos_tag` function"]},{"cell_type":"code","metadata":{"id":"YtVGv50fFvwn"},"source":["import nltk\n","nltk.download('averaged_perceptron_tagger')\n","from nltk import pos_tag\n","\n","pos_tag(train_toks)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BhpoQkfnFvwp"},"source":["### Exercise 4.1\n","Write or adapt your code so that you can evaluate `nltk pos_tag` function on the training and testing data divided above.\n"]},{"cell_type":"code","metadata":{"id":"WTid1DZ_Fvwp"},"source":["def evaluate_pos_tag(goldstandard):\n","    goldtoks,goldtags=zip(*goldstandard)\n","    pretoks,pretags=zip(*pos_tag(goldtoks))\n","    print(goldtags[:10])\n","    print(pretags[:10])\n","    n=len(pretags)\n","    correct=0\n","    for (pre,gold) in zip(pretags,goldtags):\n","        if pre==gold:\n","            correct+=1\n","    return correct/n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1cA3jmgAFvwt"},"source":["evaluate_pos_tag(train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h8OBt7P9Fvwv"},"source":["evaluate_pos_tag(test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YcT-LhWKFvwx"},"source":["### Extension\n","Find examples where the unigram tagger makes mistakes but the nltk pos tagger is correct.  What different types of errors are being made?  Can you explain intuitively why the correct sequence predicted by the nltk pos tagger is more likely than the one predicted by the unigram tagger?"]},{"cell_type":"code","metadata":{"id":"SIC5sMwjFvwy"},"source":["\n","def compare_pos_taggers(goldstandard,unigram_model):\n","    goldtoks,goldtags=zip(*goldstandard)\n","    pretoks,pretags=zip(*pos_tag(goldtoks))\n","    unitoks,unitags=zip(*uni_pos_tag(goldtoks,unigram_model))\n","    \n","    #find\n","    examples=[]\n","    for i,(gold,pre,uni) in enumerate(zip(goldtags,pretags,unitags)):\n","        if gold==pre and gold!=uni:\n","            examples.append(i)\n","        \n","    #display\n","    headers=['correct','unigram','before','after']\n","    rows=[]\n","    for e in examples:\n","        rows.append([goldstandard[e],unitags[e],goldstandard[e-3:e],goldstandard[e+1:e+3]])\n","        \n","    df=pd.DataFrame(rows,columns=headers)\n","    return df\n","\n","compare_pos_taggers(test,unigram_model)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8kE63lP_Fvw0"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zVJV7Jk_Fvw4"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VBKDRkelc8_V"},"source":["# Lecture Code for HMM Emission and Transition Probabilities"]},{"cell_type":"code","metadata":{"id":"qB3LMR1GFvw6"},"source":["def calculate_emissions(trainlist):\n","    #trainlist is a list of (word,tag) pairs\n","    emissions={}\n","    for word,tag in trainlist:\n","        current=emissions.get(tag,{})\n","        current[word]=current.get(word,0)+1\n","        emissions[tag]=current\n","    return {tag:{word:value/sum(worddist.values()) for word,value in worddist.items()} \n","            for tag,worddist in emissions.items()}\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V_CK3JyKFvw8"},"source":["calculate_emissions(train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KabkWZMyFvw_"},"source":["def calculate_transitions(trainlist):\n","    transitions={}\n","    previous=\"start\"\n","    for _, tag in trainlist:\n","        current=transitions.get(previous,{})\n","        current[tag]=current.get(tag,0)+1\n","        transitions[tag]=current\n","        previous =tag\n","    return {previous:{tag:value/sum(tagdist.values()) for tag,value in tagdist.items()} \n","            for previous,tagdist in transitions.items()}\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u4gMbmElFvxG"},"source":["calculate_transitions(train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XhUtBDXRFvxI"},"source":[""],"execution_count":null,"outputs":[]}]}