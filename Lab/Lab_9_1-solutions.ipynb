{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Lab_9_1-solutions.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"xOiYI32-eUfF"},"source":["# Week 9: Named Entity Recognition\n","\n","This week we are going to looking at named entity recognition in the fiction genre. In doing so we will introduce the spaCy library (https://spacy.io/) which provides a number of very fast, state-of-the-art accuracy tools for carrying out NLP tasks including part-of-speech tagging, dependency parsing and named entity recognition.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"raM1LTb5eUfG","executionInfo":{"status":"ok","timestamp":1605889633282,"user_tz":0,"elapsed":25799,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"39e4d655-13e0-41f9-f9a4-f32772eafe7e"},"source":["#preliminary imports\n","\n","from google.colab import drive\n","#mount google drive\n","drive.mount('/content/drive/')\n","import sys\n","sys.path.append('/content/drive/My Drive/NLE Notebooks/resources/')\n","\n","import pandas as pd\n","import operator"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pW_tPwNseUfG"},"source":["## Project Gutenberg\n","\n","[Project Gutenberg electronic text archive](http://www.gutenberg.org/) contains around 25,000 free electronic books.\n","\n","A small selection is made available through the NLTK. For the full list, run the following cell."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jrud69QPe3Qr","executionInfo":{"status":"ok","timestamp":1605889675351,"user_tz":0,"elapsed":1992,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"eea21efd-5a10-435f-f15a-9b314b01205e"},"source":["import nltk\n","nltk.download('gutenberg')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/gutenberg.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4K_tAJUFeUfG","executionInfo":{"status":"ok","timestamp":1605889677826,"user_tz":0,"elapsed":744,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"4079ebdb-4413-4d3c-9926-cacb1837a003"},"source":["from nltk.corpus import gutenberg\n","gutenberg.fileids()"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['austen-emma.txt',\n"," 'austen-persuasion.txt',\n"," 'austen-sense.txt',\n"," 'bible-kjv.txt',\n"," 'blake-poems.txt',\n"," 'bryant-stories.txt',\n"," 'burgess-busterbrown.txt',\n"," 'carroll-alice.txt',\n"," 'chesterton-ball.txt',\n"," 'chesterton-brown.txt',\n"," 'chesterton-thursday.txt',\n"," 'edgeworth-parents.txt',\n"," 'melville-moby_dick.txt',\n"," 'milton-paradise.txt',\n"," 'shakespeare-caesar.txt',\n"," 'shakespeare-hamlet.txt',\n"," 'shakespeare-macbeth.txt',\n"," 'whitman-leaves.txt']"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"nZO6Uhx0eUfI"},"source":["We can get the raw text of any of the novels using the `gutenberg.raw(fileid)` method.  This returns a String."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"id":"CK4JmiOGeUfI","executionInfo":{"status":"ok","timestamp":1605889684922,"user_tz":0,"elapsed":675,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"b52da9f2-e8ec-46e4-957e-13f9168ebdf1"},"source":["emma=gutenberg.raw('austen-emma.txt')\n","emma[:500]"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"[Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAPTER I\\n\\n\\nEmma Woodhouse, handsome, clever, and rich, with a comfortable home\\nand happy disposition, seemed to unite some of the best blessings\\nof existence; and had lived nearly twenty-one years in the world\\nwith very little to distress or vex her.\\n\\nShe was the youngest of the two daughters of a most affectionate,\\nindulgent father; and had, in consequence of her sister's marriage,\\nbeen mistress of his house from a very early period.  Her mother\\nhad died t\""]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"VDC653sjeUfJ"},"source":["Now, we carry out a little bit of cleaning of the text.  Check you understand what each line in the `clean_text()` function does."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":103},"id":"7Vwu-DWIeUfJ","executionInfo":{"status":"ok","timestamp":1605889691256,"user_tz":0,"elapsed":651,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"353ce8a1-5b75-4b2e-b6ee-d16955c82b03"},"source":["import re\n","def clean_text(astring):\n","    #replace newlines with space\n","    newstring=re.sub(\"\\n\",\" \",astring)\n","    #remove title and chapter headings\n","    newstring=re.sub(\"\\[[^\\]]*\\]\",\" \",newstring)\n","    newstring=re.sub(\"VOLUME \\S+\",\" \",newstring)\n","    newstring=re.sub(\"CHAPTER \\S+\",\" \",newstring)\n","    newstring=re.sub(\"\\s\\s+\",\" \",newstring)\n","    #return re.sub(\"([^\\.|^ ])  +\",r\"\\1 .  \",newstring).lstrip().rstrip()\n","    return newstring.lstrip().rstrip()\n","\n","clean_emma=clean_text(emma)\n","print(len(emma))\n","print(len(clean_emma))\n","clean_emma[:500]"],"execution_count":6,"outputs":[{"output_type":"stream","text":["887071\n","880067\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to unite some of the best blessings of existence; and had lived nearly twenty-one years in the world with very little to distress or vex her. She was the youngest of the two daughters of a most affectionate, indulgent father; and had, in consequence of her sister's marriage, been mistress of his house from a very early period. Her mother had died too long ago for her to have more than an indistinct \""]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"RZw5tkm0eUfJ"},"source":["## SpaCy\n","\n","If working at home, you may need to install spaCy and download a set of English models.  at the command line:\n","\n","```\n","pip install spacy\n","python -m spacy download en_core_web_sm\n","```\n","\n","In the lab, or once you have done this at home, you should then be able to set up a spaCy processing pipeline as follows. If working on colab than this should work automatically."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yuku0GwWeUfJ","executionInfo":{"status":"ok","timestamp":1605889850044,"user_tz":0,"elapsed":1716,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"2d2195ef-4284-44d5-eeb3-01793dfc1379"},"source":["import spacy\n","#nlp=spacy.load('en')\n","nlp=spacy.load('en_core_web_sm')\n","type(nlp)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["spacy.lang.en.English"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"L9FHGRuzeUfJ"},"source":["Now we can run any text string through the language processing pipeline stored in `nlp`\n","This next cell might take a few minutes to run since it carries out all of the SpaCy NLP functionality on the input text.  It will return a SpaCy `Doc` object which contains the text plus various annotations.  See the SpaCy documentation https://spacy.io/api/doc"]},{"cell_type":"code","metadata":{"id":"1z26NL3teUfJ","executionInfo":{"status":"ok","timestamp":1605889883340,"user_tz":0,"elapsed":30244,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}}},"source":["nlp_emma=nlp(clean_emma)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4AjnNopNeUfJ","executionInfo":{"status":"ok","timestamp":1605889889365,"user_tz":0,"elapsed":782,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"10c44ec6-e2aa-40ce-d32c-984c0065923a"},"source":["type(nlp_emma)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["spacy.tokens.doc.Doc"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"CM26L9EeeUfJ"},"source":["For example, we can now iterate over sentences in the document."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3M1o8boAeUfJ","executionInfo":{"status":"ok","timestamp":1605889893020,"user_tz":0,"elapsed":761,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"7b52f76d-946e-4592-a3f5-0a4cc6bd4f8e"},"source":["for s in nlp_emma.sents:\n","    print(s)\n","    break"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to unite some of the best blessings of existence; and had lived nearly twenty-one years in the world with very little to distress or vex her.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"naVVmUxeeUfJ"},"source":["We can iterate over tokens in sentences and find out the labels added by SpaCy to each token."]},{"cell_type":"code","metadata":{"id":"_zXgZk99eUfJ","executionInfo":{"status":"ok","timestamp":1605889899785,"user_tz":0,"elapsed":893,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}}},"source":["emma_sents=list(nlp_emma.sents)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V9amnqyVeUfK","executionInfo":{"status":"ok","timestamp":1605889901455,"user_tz":0,"elapsed":722,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"7303354a-8f1f-4a7b-fc94-3fa168980dc1"},"source":["print(emma_sents[0])"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to unite some of the best blessings of existence; and had lived nearly twenty-one years in the world with very little to distress or vex her.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"FT25iErbeUfK","executionInfo":{"status":"ok","timestamp":1605889909337,"user_tz":0,"elapsed":772,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"2cdb09a8-1e34-4c55-d011-0c3c03a0a306"},"source":["def display_sent(asent):\n","    headings=[\"token\",\"lower\",\"lemma\",\"pos\",\"NER\"]\n","    info=[]\n","    for t in asent:\n","        info.append([t.text,t.lower_,t.lemma_,t.pos_,t.ent_type_])\n","    return(pd.DataFrame(info,columns=headings))\n","        \n","display_sent(emma_sents[3])"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>token</th>\n","      <th>lower</th>\n","      <th>lemma</th>\n","      <th>pos</th>\n","      <th>NER</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Sixteen</td>\n","      <td>sixteen</td>\n","      <td>sixteen</td>\n","      <td>NUM</td>\n","      <td>DATE</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>years</td>\n","      <td>years</td>\n","      <td>year</td>\n","      <td>NOUN</td>\n","      <td>DATE</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>had</td>\n","      <td>had</td>\n","      <td>have</td>\n","      <td>AUX</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Miss</td>\n","      <td>miss</td>\n","      <td>Miss</td>\n","      <td>PROPN</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Taylor</td>\n","      <td>taylor</td>\n","      <td>Taylor</td>\n","      <td>PROPN</td>\n","      <td>PERSON</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>been</td>\n","      <td>been</td>\n","      <td>be</td>\n","      <td>AUX</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>in</td>\n","      <td>in</td>\n","      <td>in</td>\n","      <td>ADP</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Mr.</td>\n","      <td>mr.</td>\n","      <td>Mr.</td>\n","      <td>PROPN</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Woodhouse</td>\n","      <td>woodhouse</td>\n","      <td>Woodhouse</td>\n","      <td>PROPN</td>\n","      <td>PERSON</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>'s</td>\n","      <td>'s</td>\n","      <td>'s</td>\n","      <td>PART</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>family</td>\n","      <td>family</td>\n","      <td>family</td>\n","      <td>NOUN</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>,</td>\n","      <td>,</td>\n","      <td>,</td>\n","      <td>PUNCT</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>less</td>\n","      <td>less</td>\n","      <td>less</td>\n","      <td>ADJ</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>as</td>\n","      <td>as</td>\n","      <td>as</td>\n","      <td>SCONJ</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>a</td>\n","      <td>a</td>\n","      <td>a</td>\n","      <td>DET</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>governess</td>\n","      <td>governess</td>\n","      <td>governess</td>\n","      <td>NOUN</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>than</td>\n","      <td>than</td>\n","      <td>than</td>\n","      <td>SCONJ</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>a</td>\n","      <td>a</td>\n","      <td>a</td>\n","      <td>DET</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>friend</td>\n","      <td>friend</td>\n","      <td>friend</td>\n","      <td>NOUN</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>,</td>\n","      <td>,</td>\n","      <td>,</td>\n","      <td>PUNCT</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>very</td>\n","      <td>very</td>\n","      <td>very</td>\n","      <td>ADV</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>fond</td>\n","      <td>fond</td>\n","      <td>fond</td>\n","      <td>ADJ</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>of</td>\n","      <td>of</td>\n","      <td>of</td>\n","      <td>ADP</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>both</td>\n","      <td>both</td>\n","      <td>both</td>\n","      <td>DET</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>daughters</td>\n","      <td>daughters</td>\n","      <td>daughter</td>\n","      <td>NOUN</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>,</td>\n","      <td>,</td>\n","      <td>,</td>\n","      <td>PUNCT</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>but</td>\n","      <td>but</td>\n","      <td>but</td>\n","      <td>CCONJ</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>particularly</td>\n","      <td>particularly</td>\n","      <td>particularly</td>\n","      <td>ADV</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>of</td>\n","      <td>of</td>\n","      <td>of</td>\n","      <td>ADP</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>Emma</td>\n","      <td>emma</td>\n","      <td>Emma</td>\n","      <td>PROPN</td>\n","      <td>ORG</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>.</td>\n","      <td>.</td>\n","      <td>.</td>\n","      <td>PUNCT</td>\n","      <td></td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           token         lower         lemma    pos     NER\n","0        Sixteen       sixteen       sixteen    NUM    DATE\n","1          years         years          year   NOUN    DATE\n","2            had           had          have    AUX        \n","3           Miss          miss          Miss  PROPN        \n","4         Taylor        taylor        Taylor  PROPN  PERSON\n","5           been          been            be    AUX        \n","6             in            in            in    ADP        \n","7            Mr.           mr.           Mr.  PROPN        \n","8      Woodhouse     woodhouse     Woodhouse  PROPN  PERSON\n","9             's            's            's   PART        \n","10        family        family        family   NOUN        \n","11             ,             ,             ,  PUNCT        \n","12          less          less          less    ADJ        \n","13            as            as            as  SCONJ        \n","14             a             a             a    DET        \n","15     governess     governess     governess   NOUN        \n","16          than          than          than  SCONJ        \n","17             a             a             a    DET        \n","18        friend        friend        friend   NOUN        \n","19             ,             ,             ,  PUNCT        \n","20          very          very          very    ADV        \n","21          fond          fond          fond    ADJ        \n","22            of            of            of    ADP        \n","23          both          both          both    DET        \n","24     daughters     daughters      daughter   NOUN        \n","25             ,             ,             ,  PUNCT        \n","26           but           but           but  CCONJ        \n","27  particularly  particularly  particularly    ADV        \n","28            of            of            of    ADP        \n","29          Emma          emma          Emma  PROPN     ORG\n","30             .             .             .  PUNCT        "]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"XhdIUCsceUfK"},"source":["### Exercise 1.1\n","Run the `display_sent()` function on each of the first ten sentences of Emma (as stored in `emma_sents`).\n","* What errors do you see in the named entity recognition?\n","* Can you see any patterns in the words, lemmas or part-of-speech tags which might be used to improve the named entity recognition on these sentences?\n"]},{"cell_type":"code","metadata":{"id":"YZqWA-PeeUfK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZivY8DSmeUfK"},"source":["### Exercise 1.2\n","Write a function 'make_tag_lists()' which takes a list of sentences as input and which returns 3 lists:\n","1. tokens\n","2. POS tags\n","3. Named Entity tags\n","\n","These lists should be the same length (189191, if applied to the all of the sentences in `nlp_emma`) and maintain the order of the text, i.e., position i in each list should refer to the same token."]},{"cell_type":"code","metadata":{"id":"DxADffMReUfK","executionInfo":{"status":"ok","timestamp":1605889951162,"user_tz":0,"elapsed":803,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}}},"source":["def make_tag_lists(sents):\n","    tokens=[]\n","    pos_tags=[]\n","    ner_tags=[]\n","    for sent in sents:\n","        for t in sent:\n","            tokens.append(t.text)\n","            pos_tags.append(t.pos_)\n","            ner_tags.append(t.ent_type_)\n","    return tokens,pos_tags,ner_tags\n","toks,pos,ner=make_tag_lists(emma_sents)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3AWz7fRgeUfK","executionInfo":{"status":"ok","timestamp":1605889953265,"user_tz":0,"elapsed":728,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"67ab6052-ca50-4526-8b0a-3db07413e7d3"},"source":["print(len(toks),len(pos),len(ner))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["189191 189191 189191\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EUWG_kcTeUfK"},"source":["### Exercise 1.3\n","Write a function `extract_entities` which takes a list of tokens, a list of tags and a tag-type and returns a dictionary of all of the **chunks** which have the given tag-type; together with their frequency in the text.\n","\n","You can assume that two consecutive tokens with the same tag are part of the same chunk.\n","\n","Test your code and you should get the following output (for the given input):\n","\n","<img src=output-13.png>\n","\n","This tells us that \"Anne Cox\" is tagged twice as a named entity of type \"PERSON\" in the text.  How many occurrences of \"Miss Woodhouse\" tagged as a \"PERSON\" are there?"]},{"cell_type":"code","metadata":{"id":"eN70pZKneUfK","executionInfo":{"status":"ok","timestamp":1605889980795,"user_tz":0,"elapsed":701,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}}},"source":["def extract_entities(tokenlist,taglist,tagtype):\n","    \n","    entities={}\n","    inentity=False\n","    for i,(token,tag) in enumerate(zip(tokenlist,taglist)):\n","        if tag==tagtype:\n","            if inentity:\n","                entity+=\" \"+token\n","            else:\n","                entity=token\n","                inentity=True\n","        elif inentity:\n","            entities[entity]=entities.get(entity,0)+1\n","            inentity=False\n","    return entities           \n","            \n","        \n","    \n","    "],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"tIZGru5LeUfK","executionInfo":{"status":"ok","timestamp":1605889982522,"user_tz":0,"elapsed":1131,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}}},"source":[""],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ISRNuVVPeUfK","executionInfo":{"status":"ok","timestamp":1605889982523,"user_tz":0,"elapsed":762,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"b44263a6-e275-4664-ca9f-296789862ec9"},"source":["extract_entities(toks,ner,\"PERSON\")"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Abominable scoundrel!\"--': 1,\n"," 'Aladdin': 1,\n"," 'Anna Weston': 1,\n"," 'Anne Cox': 2,\n"," 'Arthur!--How': 1,\n"," 'Astley': 4,\n"," 'Augusta': 2,\n"," 'Augusta Hawkins': 1,\n"," 'Aye': 12,\n"," 'Baly - craig': 1,\n"," 'Bates': 142,\n"," 'Bates.--He': 1,\n"," 'Bella': 3,\n"," 'Bickerton': 2,\n"," 'Bird': 1,\n"," 'Bought': 1,\n"," 'Box Hill': 13,\n"," 'Box Hill--': 1,\n"," 'Bragge': 7,\n"," 'Bristol': 2,\n"," 'Broadwood': 2,\n"," \"Brown's\": 1,\n"," 'Campbell': 51,\n"," 'Captain Weston': 1,\n"," 'Catherine': 1,\n"," 'Churchill': 69,\n"," 'Churchill--': 1,\n"," 'Churchills': 1,\n"," 'Clara Partridge': 1,\n"," 'Cole': 51,\n"," 'Cole--': 1,\n"," 'Coles': 8,\n"," 'Cox': 4,\n"," 'Coxe': 1,\n"," 'Coxes': 3,\n"," 'Cromer': 2,\n"," 'Dear Harriet': 1,\n"," 'Dear Jane': 1,\n"," 'Dearer': 1,\n"," 'Dixon': 39,\n"," 'Donwell': 6,\n"," 'Donwell Abbey': 1,\n"," 'Donwell Lane': 1,\n"," 'E.': 11,\n"," 'Easter': 1,\n"," 'Elizabeth': 5,\n"," 'Elizabeth Martin': 1,\n"," \"Elizabeth Martin 's\": 2,\n"," 'Elton': 377,\n"," 'Elton _': 1,\n"," 'Elton!--': 1,\n"," 'Elton!--`Jane Fairfax': 1,\n"," 'Elton!--no': 1,\n"," \"Elton's\": 2,\n"," 'Elton--': 1,\n"," 'Elton.--': 1,\n"," 'Elton?--': 1,\n"," 'Eltons': 2,\n"," 'Emma': 168,\n"," 'Emma Woodhouse': 2,\n"," 'Emma Woodhouse - ing': 1,\n"," 'Emma dine': 1,\n"," 'Emma herself.--The': 1,\n"," 'Enscombe': 8,\n"," 'F. C. WESTON': 1,\n"," 'F. C. Weston Churchill': 1,\n"," 'Fairfax': 112,\n"," 'Fairfax!--Well': 1,\n"," 'Ford': 5,\n"," 'Frank': 56,\n"," 'Frank Churchill': 129,\n"," \"Frank Churchill 's\": 14,\n"," 'Frank Churchill.--At': 1,\n"," 'Frank Churchill.--He': 1,\n"," 'Garrick': 1,\n"," 'George': 5,\n"," 'George Otway': 1,\n"," 'Gilbert': 4,\n"," 'Goddard': 58,\n"," 'Graham': 1,\n"," 'Grandpapa': 1,\n"," 'Green': 1,\n"," 'Hannah': 2,\n"," 'Harriet': 425,\n"," 'Harriet Smith': 22,\n"," \"Harriet Smith 's\": 4,\n"," 'Harriet Smith!--It': 1,\n"," 'Harriet Smith!--Such': 1,\n"," 'Harriet indignantly.--\"Oh': 1,\n"," 'Harriet.--It': 1,\n"," 'Harry': 1,\n"," 'Hartfield': 17,\n"," 'Hawkins': 4,\n"," 'Hawkins!--Good': 1,\n"," 'Hazle': 1,\n"," 'He _': 1,\n"," 'Heavens': 1,\n"," 'Henry': 20,\n"," 'Hetty': 1,\n"," 'Highbury.--\"He': 1,\n"," 'Hodges': 5,\n"," 'Hughes': 3,\n"," 'Ill': 1,\n"," 'Isabella': 65,\n"," 'James': 18,\n"," 'James Cooper': 1,\n"," 'Jane': 182,\n"," 'Jane Bates': 1,\n"," 'Jane Fairfax': 85,\n"," \"Jane Fairfax 's\": 14,\n"," 'Jane Fairfax!--Good': 1,\n"," 'Jane Fairfax!--Harriet': 1,\n"," \"Jane Fairfax's.--Of\": 1,\n"," 'Jeffereys': 1,\n"," 'John': 26,\n"," \"John Abdy 's\": 1,\n"," 'John Knightley': 54,\n"," \"John Knightley 's\": 1,\n"," 'John Knightley)--your': 1,\n"," 'John Knightley.--This': 1,\n"," 'John Saunders': 1,\n"," 'John ostler': 1,\n"," 'Kindled': 1,\n"," 'Kitty': 2,\n"," 'Knightley': 283,\n"," 'Knightley marry!--No': 1,\n"," 'Knightley!--Can': 1,\n"," 'Knightley!--How': 1,\n"," \"Knightley's.--\": 1,\n"," 'Knightley--\"I': 1,\n"," 'Knightley.--': 3,\n"," 'Knightley.--\"Emma': 1,\n"," 'Knightley.--\"Robert Martin \\'s': 1,\n"," 'Knightley.--Were': 1,\n"," 'Knightley;--and': 1,\n"," 'Knightley?--Who': 1,\n"," 'Knightleys': 6,\n"," \"La Baronne d'Almane\": 1,\n"," 'Lady Patroness': 1,\n"," 'Langham': 1,\n"," 'Little Henry': 2,\n"," 'Maple Grove': 4,\n"," 'Martin': 49,\n"," 'Martin--': 1,\n"," 'Middling': 1,\n"," 'Miss Bates': 2,\n"," 'Miss Bickerton': 1,\n"," 'Miss F--': 1,\n"," 'Miss F.': 1,\n"," 'Miss Hawkins': 6,\n"," 'Miss Nash': 1,\n"," 'Miss Prince': 1,\n"," 'Miss Richardson': 1,\n"," 'Miss Smith': 6,\n"," \"Miss Smith 's\": 1,\n"," 'Miss Taylor': 1,\n"," 'Miss W.': 2,\n"," 'Miss Woodhouse': 96,\n"," \"Miss Woodhouse 's\": 10,\n"," 'Mrs _': 1,\n"," 'N. takes M.': 1,\n"," 'Nash': 12,\n"," 'No!--she': 1,\n"," 'Otway': 3,\n"," 'Papa': 3,\n"," 'Partridge': 2,\n"," 'Patty': 12,\n"," 'Peculiarly lucky!--': 1,\n"," 'Perry': 75,\n"," 'Perry!--': 1,\n"," 'Perry?--Has': 1,\n"," 'Perrys': 1,\n"," 'Philip _': 1,\n"," 'Pray': 1,\n"," 'Randall': 1,\n"," 'Randalls': 1,\n"," 'Richard?--': 1,\n"," 'Richard?--I': 1,\n"," 'Robert': 1,\n"," 'Robert Martin': 24,\n"," \"Robert Martin 's\": 3,\n"," 'Robin': 1,\n"," 'S.': 1,\n"," 'Selina': 6,\n"," 'Shakespeare': 1,\n"," 'Smallridge': 8,\n"," 'Smith': 42,\n"," 'Smith!--': 1,\n"," 'Smith!--Miss Smith': 1,\n"," 'Smith!--What': 1,\n"," 'Smiths': 1,\n"," 'Somebody': 1,\n"," 'Son': 1,\n"," 'Stilton': 1,\n"," 'Stokes': 3,\n"," 'Suckling': 14,\n"," 'Surry': 5,\n"," 'Taylor': 45,\n"," 'Theodore': 1,\n"," 'Tom': 1,\n"," 'Tunbridge': 1,\n"," 'Uncle Knightley': 1,\n"," 'W.': 1,\n"," 'Wallis': 3,\n"," 'Welch': 2,\n"," 'Weston': 412,\n"," 'Weston!--': 1,\n"," 'Weston\"--with': 1,\n"," 'Weston--': 1,\n"," 'Weston.--So': 1,\n"," 'Weston?--Judge': 1,\n"," 'Weston?--To Bath': 1,\n"," 'Westons': 3,\n"," 'Weymouth': 5,\n"," 'William': 4,\n"," 'William Cox': 2,\n"," 'William Coxe--': 1,\n"," 'William Larkins': 12,\n"," \"William Larkins 's\": 1,\n"," 'Wingfield': 9,\n"," 'Woodhouse': 145,\n"," 'Woodhouse--\"yes': 1,\n"," 'Woodhouse?--I am': 2,\n"," 'Woodhouses': 1,\n"," 'Would Jane': 1,\n"," 'Wright': 4,\n"," 'Yorkshire': 6,\n"," '_': 1,\n"," '_ Bath _': 1,\n"," '_ Gilbert': 1,\n"," 'a Harriet Smith': 1,\n"," 'again!--But': 1,\n"," 'again--\"Mr': 1,\n"," 'am--': 1,\n"," 'baker': 1,\n"," 'ball!--why': 1,\n"," 'been!--Every': 1,\n"," 'been.--Long': 1,\n"," 'comfort!--': 1,\n"," 'considerate!--But': 1,\n"," 'could!--How': 1,\n"," 'delicacy!--': 1,\n"," 'encouragement!--Sir': 1,\n"," 'entirely!--She': 1,\n"," 'feel!--Not': 1,\n"," 'frozen maid': 1,\n"," 'gallantry;--it': 1,\n"," 'glad.--Quite': 1,\n"," 'haberdasher': 1,\n"," 'hair!--': 1,\n"," 'happily!--': 1,\n"," 'hazle eyes': 1,\n"," 'headache!--': 1,\n"," 'herself.--Robert Martin': 1,\n"," 'him?--It': 1,\n"," 'house!--How': 1,\n"," 'however.--Her': 1,\n"," 'hush!--': 1,\n"," 'idea.--Did': 1,\n"," 'impair.--Perhaps': 1,\n"," 'impossibility!--You': 1,\n"," 'in--': 1,\n"," 'indeed!--(Harriet': 1,\n"," 'indeed!--You': 1,\n"," 'invention.--There': 1,\n"," 'it?--Selina': 1,\n"," \"jealousy.--In Jane 's\": 1,\n"," 'kind.--The': 1,\n"," 'kindness!--': 1,\n"," 'know?--had': 1,\n"," 'marry!--You': 1,\n"," 'marry--': 1,\n"," 'me--': 1,\n"," 'me-- Jane': 1,\n"," 'me?--Absurd': 1,\n"," 'mean!\"--': 1,\n"," 'merited!--The': 1,\n"," 'merry evening': 1,\n"," 'mutton': 1,\n"," 'obliging!--But': 1,\n"," 'of--': 2,\n"," 'others!--You': 1,\n"," 'ought!--Had': 1,\n"," 'papa': 7,\n"," 'papa!--Now': 1,\n"," 'picture!--How': 1,\n"," 'pocket!--One': 1,\n"," 'preparation!--You': 1,\n"," 'quarter:--Robert Martin': 1,\n"," 'rain!--This': 1,\n"," 'said.--Her': 1,\n"," 'shark': 1,\n"," 'so-- Jane': 1,\n"," 'story.--Robert Martin': 1,\n"," 'suffered.--Pilfering': 1,\n"," 'talents!--I': 1,\n"," 'woman!--But': 1,\n"," 'ye': 1,\n"," 'you--(nodding': 1,\n"," 'you?--Here': 1}"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"IfoTGHdPeUfK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jx75rl5UeUfK"},"source":["### Exercise 1.4\n","Use your code to find \n","* the 20 most commonly referred to people in Emma\n","* the 20 most commonly referred to places in Emma"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wMu4JvTPeUfK","executionInfo":{"status":"ok","timestamp":1605890002827,"user_tz":0,"elapsed":944,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"61ae6e86-18ae-4832-e259-24e1942d3dce"},"source":["people=extract_entities(toks,ner,\"PERSON\")\n","top_people=sorted(people.items(),key=operator.itemgetter(1),reverse=True)[:100]\n","print(top_people)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["[('Harriet', 425), ('Weston', 412), ('Elton', 377), ('Knightley', 283), ('Jane', 182), ('Emma', 168), ('Woodhouse', 145), ('Bates', 142), ('Frank Churchill', 129), ('Fairfax', 112), ('Miss Woodhouse', 96), ('Jane Fairfax', 85), ('Perry', 75), ('Churchill', 69), ('Isabella', 65), ('Goddard', 58), ('Frank', 56), ('John Knightley', 54), ('Cole', 51), ('Campbell', 51), ('Martin', 49), ('Taylor', 45), ('Smith', 42), ('Dixon', 39), ('John', 26), ('Robert Martin', 24), ('Harriet Smith', 22), ('Henry', 20), ('James', 18), ('Hartfield', 17), (\"Frank Churchill 's\", 14), (\"Jane Fairfax 's\", 14), ('Suckling', 14), ('Box Hill', 13), ('Nash', 12), ('Aye', 12), ('Patty', 12), ('William Larkins', 12), ('E.', 11), (\"Miss Woodhouse 's\", 10), ('Wingfield', 9), ('Enscombe', 8), ('Coles', 8), ('Smallridge', 8), ('papa', 7), ('Bragge', 7), ('Yorkshire', 6), ('Miss Smith', 6), ('Knightleys', 6), ('Miss Hawkins', 6), ('Donwell', 6), ('Selina', 6), ('Elizabeth', 5), ('George', 5), ('Surry', 5), ('Weymouth', 5), ('Ford', 5), ('Hodges', 5), (\"Harriet Smith 's\", 4), ('Hawkins', 4), ('Cox', 4), ('William', 4), ('Gilbert', 4), ('Maple Grove', 4), ('Wright', 4), ('Astley', 4), (\"Robert Martin 's\", 3), ('Bella', 3), ('Papa', 3), ('Westons', 3), ('Coxes', 3), ('Wallis', 3), ('Stokes', 3), ('Hughes', 3), ('Otway', 3), ('Knightley.--', 3), ('Emma Woodhouse', 2), ('Hannah', 2), ('Welch', 2), ('Eltons', 2), ('Kitty', 2), ('Cromer', 2), ('of--', 2), ('Miss Bates', 2), (\"Elizabeth Martin 's\", 2), ('Broadwood', 2), ('Little Henry', 2), ('Anne Cox', 2), ('William Cox', 2), ('Woodhouse?--I am', 2), ('Bristol', 2), ('Augusta', 2), ('Partridge', 2), (\"Elton's\", 2), ('Bickerton', 2), ('Miss W.', 2), ('Captain Weston', 1), ('Donwell Abbey', 1), ('Woodhouses', 1), ('a Harriet Smith', 1)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bXKOF2xMeUfK","executionInfo":{"status":"ok","timestamp":1605890004904,"user_tz":0,"elapsed":682,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"46473ce3-601e-4418-d5aa-ec1eafc17c28"},"source":["places=extract_entities(toks,ner,\"GPE\")\n","top_places=sorted(places.items(),key=operator.itemgetter(1),reverse=True)[:20]\n","print(top_places)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["[('Hartfield', 131), ('London', 44), ('Enscombe', 15), ('Ireland', 14), ('Richmond', 13), ('Highbury', 8), ('Kingston', 8), ('England', 8), ('Fairfax', 7), ('Harriet', 5), ('Maple Grove', 5), ('Surry', 4), ('Woodhouses', 3), ('Emma Woodhouse', 2), ('thing?--why', 2), ('Weymouth', 2), ('Serle', 2), ('Selina', 2), ('us', 2), ('Birmingham', 2)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KHRM_BgmeUfK","executionInfo":{"status":"ok","timestamp":1605890007539,"user_tz":0,"elapsed":762,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"5284298f-d3b1-4edc-b145-63ebe8a119d6"},"source":["places=extract_entities(toks,ner,\"LOC\")\n","top_places=sorted(places.items(),key=operator.itemgetter(1),reverse=True)[:20]\n","print(top_places)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["[('South End', 5), ('earth', 4), ('Eltons', 4), ('Bateses', 2), ('Maple Grove', 2), ('Windsor', 2), ('Harriet', 1), ('Miss', 1), ('can;--', 1), ('preparation;--', 1), ('Aunt Emma', 1), ('it!--It', 1), ('Emma', 1), ('stopped.--The', 1), ('extent.-- Harriet', 1), ('Box Hill', 1), ('the least.--Poor', 1)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IqEDPhBueUfL","executionInfo":{"status":"ok","timestamp":1605890015001,"user_tz":0,"elapsed":753,"user":{"displayName":"Julie Weeds","photoUrl":"","userId":"13844540934373660130"}},"outputId":"b9a20084-6b15-4729-dae4-8037d75f73b4"},"source":["set(ner)"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'',\n"," 'CARDINAL',\n"," 'DATE',\n"," 'EVENT',\n"," 'FAC',\n"," 'GPE',\n"," 'LANGUAGE',\n"," 'LOC',\n"," 'MONEY',\n"," 'NORP',\n"," 'ORDINAL',\n"," 'ORG',\n"," 'PERSON',\n"," 'PRODUCT',\n"," 'QUANTITY',\n"," 'TIME',\n"," 'WORK_OF_ART'}"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"qyvwBWq_eUfL"},"source":["### Exercise 1.5\n","Look at the lists of people and places generated.  Assuming no knowledge of the characters and plot of Emma, what errors can you see?"]},{"cell_type":"code","metadata":{"id":"5qsv0lqWeUfL"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LvzZC6QkeUfL"},"source":["## Extensions\n","\n","Code one or more of the following extensions.  In all cases, compare the lists of most frequently occurring named entities generated with the original ones.\n","\n","### Expanding NER Chunks\n","* if the word immediately before or after a named entity chunk is POS-tagged as a PROPN, assume that this word is also part of the named entity chunk\n","\n","For example, where the token \"Miss\" has pos-tag \"PROPN\" and is immediately followed by a token labelled with \"PERSON\", then it should also be labelled with \"PERSON\". \n","\n","### Relabelling NER Chunks\n","* if a named entity occurs more frequently elsewhere in the text as a different type, assume that it has been mis-labelled here\n","\n","For example, all 9 occurrences of \"Jane Fairfax\" labelled as \"GPE\" could be relabelled as \"PERSON\".\n","\n","### Linking NEs\n","* find candidates for named entity linking.  \n","\n","For example, \"Churchill\" and \"Frank Churchill\" and \"Frank\" might all refer to the same person.\n","However, you should proceed with care.  Anyone who knows the story well would tell you that \"Knightley\" and \"John Knightley\" do not refer to the same character (they are brothers).  As a further extension, give your linking functionality access to a list of known characters e.g., from https://www.janeausten.org/emma/cast-of-characters.asp\n","\n","### Co-occurring NEs\n","* find NEs that tend to co-occur together.\n","\n","Can you find pairs of named entities which often occur together (or even better, occur more often together than one would expect if named entities occur independently)?  You could consider pairs of people or alternatively co-occurrences of people and places.\n","\n","### NEs over Time\n","* record the position in the text of each named entity occurrence\n","* make a plot showing how the amount of occurrences of a given named entity varies with position in the text\n","\n","If you store each text position in `list_of_indices`, you could use:\n","`pd.Series(np.histogram(list_of_indices, bins=num_bins)` to help you with this\n"]},{"cell_type":"code","metadata":{"id":"3mCHxegseUfL"},"source":[""],"execution_count":null,"outputs":[]}]}